[{"uri":"https://huyle001.github.io/Hugo/en/1-worklog/","title":"Worklog","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nWeek 1: Introduction \u0026amp; Environment Setup\nUnderstanding Cloud computing models, Regions, and Availability Zones. Creating and configuring AWS account. Introduction to Free Tier, cost management, and Budget alerts. Getting familiar with AWS Management Console and AWS CLI.\nWeek 2: Compute \u0026amp; Networking Basics\nServices: EC2, Elastic IP, Auto Scaling, VPC basics. Network components: Subnet, Route Table, Security Group, NACL. Hands-on: Deploying a web server on EC2 accessible from the internet.\nWeek 3: Storage \u0026amp; Database\nServices: S3, EBS, EFS, RDS. Differentiating block / object / file storage. Hands-on: Creating S3 bucket with versioning \u0026amp; lifecycle, connecting application to RDS.\nWeek 4: IAM \u0026amp; Basic Security\nServices: IAM Users, Groups, Roles, Policies, MFA. AWS account security best practices. Hands-on: Creating users following least privilege principle, testing permissions.\nWeek 5: Monitoring \u0026amp; Basic Automation\nServices: CloudWatch, CloudTrail, SNS, EventBridge. Introduction to Infrastructure as Code (IaC) – CloudFormation basics. Hands-on: Creating EC2 + S3 Stack using CloudFormation.\nWeek 6: Foundation Review + Mini Project\nReview of all services: EC2, S3, RDS, IAM, CloudWatch. Deploying mini project: Static website or small web app using S3 + EC2 + RDS. Result: Solid foundation project and knowledge before entering specialized tracks.\nWeek 7: DevOps \u0026amp; Automation\nCI/CD with CodeCommit, CodeBuild, CodeDeploy, CodePipeline. Advanced IaC: CloudFormation, AWS CDK, Terraform (optional). Containers: ECS, ECR, EKS. Hands-on: Automating web app build \u0026amp; deployment using CodePipeline.\nWeek 8: Container \u0026amp; Orchestration\nDeep dive into container services: ECS, EKS, Fargate. Container networking and service discovery. Hands-on: Deploying containerized applications with ECS and EKS.\nWeek 9: Security \u0026amp; Compliance\nZero Trust architecture. Key management with AWS KMS, Secrets Manager. S3 data security, logging, and encryption. Hands-on: Setting up security workflow with IAM + KMS + CloudTrail.\nWeek 10: Serverless \u0026amp; Modern Applications\nServerless services: Lambda, API Gateway, DynamoDB, AppSync. Building event-driven architectures. Hands-on: Mini project - Serverless API or real-time application.\nWeek 11: Final Project Development\nDesigning complete architecture (front-end → backend → database). Applying CI/CD, security, monitoring, and cost optimization. Result: Completed real-world project ready for CV portfolio.\nWeek 12: Project Showcase \u0026amp; Career Prep\nWriting blog post / tech talk to share project experience. Reviewing and practicing for AWS Certified Cloud Practitioner exam. Preparing CV, LinkedIn profile, and cloud interview skills. Participating in FCJ demo day / showcase to present the final product.\n"},{"uri":"https://huyle001.github.io/Hugo/en/3-blogstranslated/3.1-blog1/","title":"Authentication for Mobile Games","tags":[],"description":"","content":" Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nAUTHENTICATION FOR MOBILE GAMES Original Article: Authentication for Mobile Games\nOriginal Author: Carl Prescott\nPublication Date: 04 OCT 2023\nINTRODUCTION User authentication and authorization is a critical aspect of almost any application. For mobile games in particular, managing the authentication and authorization of your players can pose some unique challenges. Some of these challenges relate to the use of mobile devices themselves. Other challenges relate to securing the many backend systems a modern mobile game may interact with, such as game servers, social networks, and payment providers.\nThis blog post looks to identify some of those challenges as well as providing some approaches and good practice when addressing them.\nAUTHENTICATION AND AUTHORIZATION Firstly, a quick word on the difference between authentication and authorization.\nAuthentication (often shortened to AuthN) is the act of confirming a user or system\u0026rsquo;s identity. For example, by logging in with a username and password, scanning a fingerprint, or by using facial recognition.\nAuthorization (or AuthZ) is the process of determining what permissions the user or systems linked to an authenticated identity have and whether actions they perform should be allowed or denied.\nMOBILE GAME AUTH CHALLENGES Some of the common challenges we see with authentication and authorization implementation in mobile games are:\nAnonymous user support: Mobile games frequently allow anonymous or unauthenticated players to play without creating an account. Games therefore need to track and control the access of unauthenticated players as well as associate them with an authenticated account if a player later creates an account.\nCreating simple sign-up processes: Filling in forms or providing lots of user data via mobile touchscreens can create friction for new players, resulting in them becoming frustrated and abandoning the process entirely.\nOffline authentication: Mobile devices may not always have internet connectivity. In this case an approach to local authentication or authorization support needs to be implemented.\nCaching of credentials: Care needs to be taken with locally cached credentials to ensure they can not be accessed maliciously without authorization. These credentials could be used to take non-permitted actions, for example, directly accessing APIs used by the game.\nClient tampering: Modified binaries might be used by hackers or cheaters to obtain security credentials and take unwanted actions against the game backend, or to award themselves gold or experience in-game without being authorized to do so.\nOutdated or insecure cryptography: Many mobile devices run outdated operating systems which no longer receive security updates. This can lead to poor device cryptography or other security issues.\nSpiky or unpredictable player traffic: Game launches, in-game events, promotions within the mobile game stores, or even social media activity can result in significant short term increases in traffic which may temporarily overwhelm authentication services.\nAuthentication and authorization can be a complex topic. There are serious consequences if they are not implemented correctly, so it may be advisable to use a third-party Identity-as-a-Service (IDaaS) to take care of the key functionality. Some considerations in this area are explored in the next section.\nWHAT TO LOOK FOR IN AN AUTHENTICATION SOLUTION Fundamentally an authentication system will be providing security for your players\u0026rsquo; accounts. A solution must support modern encryption mechanisms, both for data at rest and in transit. HTTPS is a must, and you should check the TLS version (you shouldn\u0026rsquo;t be using anything less than TLS v1.2). iOS has App Transport Security which prevents any outbound connections using anything less than TLS v1.2. Android has a similar capability but this can be overridden. Find out how your players\u0026rsquo; data is stored in the authentication system, passwords shouldn\u0026rsquo;t be stored directly, and any hashed values should be salted and hashed.\nAn authentication system should provide a good user experience, both for the developer and the player. It should be easy to register, sign-in and reset passwords. Social login integration, for platforms like Facebook, Twitter, Apple ID or others, that can make the sign-up and sign-in processes simpler for players is also something to consider.\nThe system should be scalable to meet the peak requests. If you have a successful launch, or some event drives a lot of users (existing or new) to your game, you need to be confident that the authentication system can scale to meet the sign-in and sign-up demand. For hosted services, look for quota/API limits, and for on premises systems, consider how the infrastructure will need to be sized to meet peak demand. Plan for peaks over and above your normal traffic patterns. For example, if your game is organically listed in the Google Play or Apple App Store New Games or Most Downloaded chart, this can result in significant additional player traffic.\nIf you need multi-platform support, look for systems that support cross-platform functionality that allows players to use the same account across different devices. This includes reviewing what SDKs are available for platforms such as iOS and Android, and for programming languages such as C# and JavaScript.\nYou will also need to think about the authentication flows you want to use. Do you need to use OAuth/OpenID flows such as authentication code or device grant? Do you want to implement a passwordless flow, such as fingerprint or facial recognition?\nLook at the system\u0026rsquo;s customisation options. You may want to build a custom sign-up or sign-in page, or use a pre-built page provided by the system. It should integrate with other systems that hold relevant player data and you should consider what options are available if you need to customise the either the authentication flow or the look-and-feel.\nCheck the analytics and reporting capability to see what insight you can get from player behaviour. Can you review the login patterns, identify suspicious activity and use this information to improve your game\u0026rsquo;s security?\nMake sure you understand the cost model and choose the most appropriate fit for your expected volume. There are options that allow you to pay for what you use (for example, per sign-in/sign-out request, or by number of accounts created), and other systems require an upfront investment. Each option has its pros and cons. The most appropriate fit will depend on your budget, expected volumes, and risk appetite.\nIf you have compliance requirements to adhere to, such as data protection or age verification, make sure the authentication system is compliant, or allows you to build a solution that is compliant with the correct implementation.\nGOOD PRACTICES Some other key good practices to follow when planning your authentication and authorization approach for your mobile games are:\nUtilize the protection mechanisms provided by the app stores themselves. Google offer the Play Integrity API and Apple offers DeviceCheck, both of which you can use to ensure a player is using a genuine, unmodified game binary to access your backend server resources.\nTake advantage of the device\u0026rsquo;s hardware secure element for locally storing any cryptographic secrets or cached credentials. For Android you should use Keystore, and for iOS you should use Keychain.\nEnsure you periodically authenticate against your backend services. You may need to sometimes trust cached authentication credentials (for example, where a user has no internet connectivity), but you should avoid trusting local-only validation for extended periods of time.\nConsider using two-factor authentication (2FA) for actions which require a higher level of authorization, such as in-app purchases or changes to user data. This could involve sending a code to a known email or phone number for the user to enter in-game. Alternatively, you could implement step-up auth using a device\u0026rsquo;s biometric functionality, such as fingerprint or face ID.\nOAuth 2.0 requests should be made through external user agents, such as the device\u0026rsquo;s browser, rather than directly from the game client. This prevents the host app (or any modified apps) from being able to copy or extract user credentials and cookies, and reduces the need for the user to authenticate separately in each app they use.\nDon\u0026rsquo;t store any Personally Identifiable Information (PII) in authentication tokens. Tokens are signed but not necessarily encrypted, so they can be easily read. Including PII inside a token could result in the sensitive data being inadvertently exposed.\nFigure 1: Example authentication flow\nFigure 1 demonstrates a high-level authentication flow. The steps involved are as follows:\nThe user of the mobile app authenticates with their chosen identity provider (such as Google, Apple, Facebook, or others) which returns a token. This token is sent to the identity service, which checks that they are trusted, and if so, a token is returned to the app which can be used to log the user in to the app. Optionally, the token is then used to authorize the app to make calls to backend services. CONCLUSION In summary, we have discussed the difference between authentication (the act of confirming identity) and authorization (providing access to resources). We also explored the main challenges of mobile game authentication, what to look for in a 3rd party authentication solution and some good practices.\nIn our next blog post in this series, we\u0026rsquo;ll be looking at how you can use AWS services, such as Amazon Cognito, to build authentication into your mobile game.\nFor further information, check out the following resources:\nAWS Well Architected Framework Games Industry Lens identity and access management section Guidance for Custom Game Backend Hosting on AWS Using Amazon Cognito to authenticate players for a game backend service Blog authors:\nCarl Prescott AWS Games Solutions Architect James Thompson AWS Senior Solutions Architect ABOUT THE AUTHOR Carl Prescott is a Solutions Architect focusing on gaming customers and use cases at AWS. He started gaming on a Commodore Plus/4 far too many years ago and never really stopped. Carl brings his passion for the industry to his role where he helps game developers Build, Run and Grow their games in the cloud using the wide variety of services AWS offers.\n"},{"uri":"https://huyle001.github.io/Hugo/en/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nAI/ML/GENAI ON AWS WORKSHOP Event Information Date \u0026amp; Time: Saturday, November 15, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent Purpose The workshop was designed to provide hands-on experience with AWS AI/ML services, focusing on Amazon SageMaker for traditional machine learning workflows and Amazon Bedrock for generative AI applications. The event aimed to help participants understand the practical implementation of AI/ML solutions on AWS and explore the latest capabilities in generative AI.\nAgenda Overview 8:30 – 9:00 AM | Welcome \u0026amp; Introduction Participant registration and networking opportunities Workshop overview and learning objectives presentation Ice-breaker activity to foster collaboration Overview of the AI/ML landscape in Vietnam 9:00 – 10:30 AM | AWS AI/ML Services Overview Amazon SageMaker – End-to-End ML Platform\nData Preparation and Labeling: Understanding how to prepare datasets for machine learning, including data cleaning, feature engineering, and automated labeling capabilities Model Training, Tuning, and Deployment: Exploring SageMaker\u0026rsquo;s training infrastructure, hyperparameter tuning, and model deployment options including real-time and batch inference Integrated MLOps Capabilities: Learning about SageMaker\u0026rsquo;s built-in MLOps features for model versioning, monitoring, and automated retraining pipelines Live Demo: SageMaker Studio Walkthrough\nThe demonstration showcased the unified development environment for machine learning, including:\nJupyter notebook integration for interactive development Experiment tracking and model registry Visual workflow builder for MLOps pipelines Integration with other AWS services for data processing 10:30 – 10:45 AM | Coffee Break Networking session with refreshments and informal discussions about AI/ML use cases.\n10:45 AM – 12:00 PM | Generative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan – Comparison \u0026amp; Selection Guide\nUnderstanding different foundation models available on Bedrock Comparison of model capabilities, use cases, and performance characteristics Best practices for selecting the right model for specific business needs Cost considerations and optimization strategies Prompt Engineering: Techniques, Chain-of-Thought Reasoning, Few-shot Learning\nPrompt Engineering Fundamentals: Learning how to craft effective prompts to get desired outputs from language models Chain-of-Thought Reasoning: Understanding how to guide models through step-by-step reasoning processes for complex problem-solving Few-shot Learning: Techniques for providing examples to improve model performance on specific tasks without fine-tuning Retrieval-Augmented Generation (RAG): Architecture \u0026amp; Knowledge Base Integration\nRAG Architecture Overview: Understanding how RAG combines retrieval of relevant information with generative capabilities Knowledge Base Integration: Learning to connect Bedrock with vector databases and knowledge bases (Amazon OpenSearch, Amazon Kendra) Implementation Patterns: Best practices for building RAG applications that provide accurate, context-aware responses Bedrock Agents: Multi-step Workflows and Tool Integrations\nAgent Architecture: Understanding how Bedrock Agents can orchestrate complex multi-step workflows Tool Integration: Learning to connect agents with external APIs, databases, and AWS services Workflow Design: Patterns for designing agent-based applications that can handle complex user requests Guardrails: Safety and Content Filtering\nContent Safety: Understanding Bedrock Guardrails for filtering harmful or inappropriate content Custom Policies: Learning to configure custom content filters based on business requirements Compliance and Governance: Best practices for ensuring AI applications meet regulatory and ethical standards Live Demo: Building a Generative AI Chatbot using Bedrock\nThe demonstration walked through creating a complete chatbot application:\nSetting up Bedrock foundation model Implementing RAG with knowledge base integration Configuring Bedrock Agents for multi-turn conversations Adding Guardrails for content safety Deploying the chatbot application Key Highlights Comprehensive ML Platform: SageMaker provides a complete end-to-end solution for machine learning, from data preparation to model deployment and monitoring Generative AI Capabilities: Amazon Bedrock offers access to multiple foundation models, making it easy to experiment and choose the right model for each use case RAG Architecture: The RAG pattern enables building AI applications that can access and utilize specific knowledge bases, improving accuracy and relevance Production-Ready MLOps: SageMaker\u0026rsquo;s integrated MLOps capabilities simplify the process of deploying and maintaining ML models in production Safety First: Bedrock Guardrails ensure that generative AI applications are safe, compliant, and aligned with business values Key Learnings SageMaker Studio provides a unified interface for the entire ML lifecycle, significantly improving developer productivity Foundation model selection is crucial and depends on specific use cases, performance requirements, and cost constraints Prompt engineering is a critical skill that can dramatically improve model outputs without requiring fine-tuning RAG architecture is essential for building AI applications that need access to specific, up-to-date information Bedrock Agents enable building sophisticated AI applications that can handle complex, multi-step workflows Content safety must be considered from the beginning when building generative AI applications Application to My Work Experiment with SageMaker: Set up SageMaker Studio to explore ML model development for data analysis projects Build RAG Applications: Implement RAG architecture using Bedrock and knowledge bases for internal documentation and Q\u0026amp;A systems Prompt Engineering Practice: Develop prompt engineering skills by creating templates and best practices for common use cases MLOps Integration: Apply SageMaker\u0026rsquo;s MLOps capabilities to automate model training and deployment pipelines Safety Implementation: Integrate Bedrock Guardrails into any generative AI applications to ensure content safety Personal Experience This workshop provided an excellent hands-on introduction to AWS AI/ML services:\nThe SageMaker Studio demo was particularly impressive, showing how a unified platform can streamline the entire ML workflow Learning about RAG architecture was eye-opening, as it demonstrated how to build AI applications that can leverage specific knowledge bases The Bedrock Agents demonstration showed the potential for building sophisticated AI applications that can handle complex workflows The practical focus on prompt engineering provided immediately applicable skills for working with language models Understanding Guardrails helped me appreciate the importance of safety and compliance in AI applications Takeaways Start with Use Cases: Always begin by identifying specific business problems before selecting AI/ML solutions Foundation Models are Powerful: Pre-trained foundation models can solve many problems without custom training RAG is Essential: For applications requiring specific knowledge, RAG architecture is the way to go MLOps Matters: Proper MLOps practices are crucial for maintaining ML models in production Safety Cannot be Overlooked: Content filtering and safety measures must be integrated from the start Continuous Learning: The AI/ML landscape evolves rapidly, requiring continuous learning and experimentation Event Photos "},{"uri":"https://huyle001.github.io/Hugo/en/","title":"Internship Report","tags":[],"description":"","content":"Internship Report ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Le Nguyen Diep Huy\nPhone Number: 0909036854\nEmail: huylndse182703@fpt.edu.vn\nUniversity: Ho Chi Minh City University of Technology and Education\nMajor: Software Engineer\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 30/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://huyle001.github.io/Hugo/en/3-blogstranslated/3.4-blog4/","title":"Monitoring Server Health with Amazon GameLift Servers","tags":[],"description":"","content":" Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nMONITORING SERVER HEALTH WITH AMAZON GAMELIFT SERVERS Original Article: Monitoring server health with Amazon GameLift Servers\nOriginal Author: Brian Schuster\nPublication Date: 20 NOV 2025\nRunning a successful multiplayer game means constantly balancing performance, scale, and player experience. As your player base grows, new challenges emerge. The causes could be any one of a variety of different problems. Perhaps it\u0026rsquo;s something in your server code. There could be a memory leak, inefficient logic, or a bug that only surfaces under specific conditions.\nAlso, it is important to optimize latency using Amazon GameLift Servers by strategically placing capacity across Regions, verifying players connect to servers closest to them. However, even with latency optimizations in place, you might still receive complaints from players about poor performance or degraded gameplay from a subset of players.\nWe\u0026rsquo;ve previously covered how to diagnose and address wider network issues, but latency metrics alone won\u0026rsquo;t tell you the full story. You need deeper visibility into what\u0026rsquo;s happening on your game servers.\nThis is where the Amazon GameLift Servers telemetry metrics can help.\nBEYOND LATENCY: SERVER-SIDE OBSERVABILITY When using Amazon GameLift Servers with telemetry enabled, your game servers capture a variety of detailed metricsincluding resource utilization (such as CPU, memory, and network), game-specific metrics (such as tick rate), and custom metrics you define. These metrics are captured with different dimensions and levels of aggregation, so you can analyze performance at the process, container, host, and fleet-wide level. The metrics flow into your choice of metrics warehouse and can be visualized with your preferred tools and services.\nWe will demonstrate using Amazon Managed Grafana with pre-built dashboards. The set up is streamlined and can be accomplished in under an hour.\nThe real power isn\u0026rsquo;t only in the metrics themselves, it\u0026rsquo;s in how the dashboards help you quickly identify and diagnose issues.\nTROUBLESHOOTING GAME SERVER CRASHES Let\u0026rsquo;s start with a common scenario: Game sessions are crashing. Players are being disconnected mid-match, and you need to figure out why.\nIf you haven\u0026rsquo;t already set up monitoring dashboards, follow the Configure Amazon Grafana documentationit takes a few minutes to provision pre-built dashboards for your fleet. Note that we are following the C++ SDK steps linked from the Implementation documentation, adjust for your chosen implementation option.\nOnce configured, open Amazon Managed Grafana from the Amazon Web Services (AWS) Management Console (console) and navigate to the EC2 Fleet Overview dashboard. Figure 1 shows a crash event in the Game Server Crashes graph, while the Crashed Game Sessions section shows the details of the specific game session which crashed. Both the instance and the game session which crashed are conveniently linked to allow for deeper investigation.\nFigure 1: EC2 Fleet Overview dashboard showing a crashed Game Session.\nSelect the affected instance by clicking on it. The memory graph (Figure 2) illustrates the story: memory usage spiked sharply, then dropped as the process crashed. This is the signature of a memory leak. Looking at the session-level breakdown, you can see that one game session had significantly more memory usage.\nFigure 2: Instance Performance dashboard showing memory leak.\nClicking on the crashed game session brings us to the Server Performance dashboard (Figure 3), which shows the game session\u0026rsquo;s consumption up to the moment of crashing. The dashboard illustrates that the crashed game session was the one responsible for the memory leak.\nFigure 3: Server Performance dashboard showing memory leak.\nEvery graph includes helpful tooltips explaining what to look for and how to interpret the data. In this case, the next step is clear: investigate the game session logs of the crashed session to identify what triggered the leak. This can help you determine if it was a specific game mode or could have been from a particular player\u0026rsquo;s action. The metrics help point you to the right logs to examine.\nTROUBLESHOOTING HIGH CPU USAGE Let\u0026rsquo;s look at another scenario: Players are reporting stuttering gameplay, but no crashes. The issue might be CPU-related.\nSwitch to the EC2 Instances Overview dashboard in Amazon Managed Grafana. Figure 4 displays the top 20 EC2 instances by CPU consumption. Most instances hover around 2-3% CPU usage, but a few are showing at 20-30% or higher.\nFigure 4: EC2 Instances Overview dashboard.\nSelect one of the high-CPU instances. The dashboard will break down the CPU usage by game session (Figure 5) and immediately indicates which session is consuming the most resources. You can then consult the game session logs for that specific session, focusing on the period with elevated CPU usage.\nFigure 5: Instance Performance dashboard showing top CPU consuming game sessions.\nPerhaps you discover the high CPU correlates with intense combat scenarios, or maybe there\u0026rsquo;s a pathfinding bug that causes excessive calculations. While the metrics alone don\u0026rsquo;t tell you exactly what\u0026rsquo;s wrong, they do tell you exactly where to look.\nCONTAINER SUPPORT If you\u0026rsquo;re running your Amazon GameLift Servers Fleet with containers instead of EC2 instances, the same troubleshooting approach applies. Figure 6 shows the Container Fleet Overview dashboard, which displays the top tasks by CPU or memory consumption.\nFigure 6: Container Fleet Overview dashboard.\nClick on a specific task, and the Container Performance dashboard will break down the metrics by individual containers within that task (Figure 7). You can see if the game server container (Container1 in this case) is consuming resources as expected, or if a sidecar container is causing issues. This granularity helps you isolate problems quickly, whether you\u0026rsquo;re running on EC2 or containers.\nFigure 7: Container Performance dashboard.\nNEXT STEPS While the built-in metrics cover hardware performance and generic game metrics (such as tick rate and crashes), you can extend monitoring even further. Game-specific health indicators can be captured using custom metrics as described in the Custom metrics documentation.\nConsider tracking metrics such as:\nCombat balance: Average time-to-kill or damage-per-second across weapon types to detect if a recent balance patch made certain weapons overpowered Progression blockers: Success rate for critical quests or boss encounters to identify if a bug is preventing players from advancing Economy health: Currency inflation rates or item acquisition patterns to spot exploits before they destabilize your in-game economy AI pathfinding duration: Time spent calculating NPC movement paths to detect if complex scenarios are causing performance Once you\u0026rsquo;ve instrumented custom metrics that make sense for your game, set up alerts in Amazon Managed Grafana for both system and game-specific thresholds. Add alerts for memory usage above 90%, CPU usage sustained above 85%, crashed session increases, or other anomalies in your custom metrics (such as a sudden spike in failed boss attempts). When an alert fires, you can click directly into the relevant dashboard and start investigatingcatching regressions before they reach social media.\nCONCLUSION Optimizing latency can connect players to the right location, but server-side observability confirms they have a smooth experience once they\u0026rsquo;re in-game. Amazon GameLift Servers telemetry metrics, with pre-built dashboards, provides the visibility needed to quickly diagnose crashes, performance bottlenecks, and resource issueswithout drowning in raw metrics.\nThe next time a player complains about a glitchy experience, you\u0026rsquo;ll know exactly where to look, and with proactive alerting, you\u0026rsquo;ll catch the issue before they even notice.\nContact an AWS Representative to know how we can help accelerate your business.\nFURTHER READING Getting started with Amazon GameLift Servers Setting up alerts in Amazon Managed Grafana Available Amazon GameLift Servers dashboards and metrics ABOUT THE AUTHOR Brian Schuster is a Principal Engineer at AWS for Amazon GameLift where he works on shaping the technical direction of the service. He has a deep focus on driving improvement in areas of availability and scalability in order to support the most demanding requirements of large-scale games.\n"},{"uri":"https://huyle001.github.io/Hugo/en/3-blogstranslated/3.5-blog5/","title":"Strengthen Foundation Model Queries Through Amazon Bedrock-Amazon Alexa Integration","tags":[],"description":"","content":" Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSTRENGTHEN FOUNDATION MODEL QUERIES THROUGH AMAZON BEDROCK-AMAZON ALEXA INTEGRATION Original Article: Strengthen foundation model queries through Amazon Bedrock-Amazon Alexa integration\nOriginal Authors: Cristiano Scandura and Gabriel Martini\nPublication Date: 17 JAN 2025\nToday, generative artificial intelligence (AI) is at the core of many of the decisions and technologies businesses across the globe are implementing. The linchpin to ensuring that generative AI is effective depends on the foundation models (FMs) storing the data. To make sure that those FMs are pulling the correct data, Amazon Web Services (AWS) has developed a solution through Amazon Bedrock to generate SQL to answer user questions about the data. The solution was developed to support the Federal Institute of São Paulo (IFSP) and has optimized their decision-making.\nGenerative AI is based on large FMs, which are trained on vast amounts of data and are capable of generating text, images, audio, code, and much more. However, this data may be outdated or lack context, and these models have difficulty processing structured data and generating more deterministic answers. This difficulty can result in hallucinations, where the model generates outputs not based on the input data or factual knowledge. For example, when trying to sum values in a table, the model may produce random or inconsistent numbers instead of performing the calculations correctly.\nThis inability to properly interpret the structure of rows, columns, and cells, as well as the complex relationships between them, can lead to errors in identifying maximum, minimum, and other mathematical operations. Hallucination and inaccuracy issues are especially critical when dealing with sensitive data, such as financial, medical, or scientific information, where accuracy is essential, compromising the reliability of an application that uses generative AI.\nOne way to deal with the hallucination problem when answering questions in the context of structured data is to use the models to generate SQL queries. Instead of trying to directly interpret tables and perform mathematical operations, a trained model can be used to generate SQL queries based on natural language.\nAnthropic created Claude 3, an AI assistant capable of interpreting large context windows. Using prompt engineering techniques, the model can combine natural language understanding with specialized knowledge in SQL and database structures. To assist users, Anthropic has a library of prompts for different use cases, including the SQL sorcerer prompt for generating SQL queries. With these prompts, users can generate a SQL code with natural language, orchestrate the execution of the query using database services, and generate a user-friendly response. The following diagram shows the high-level architecture of the solution.\nFigure 1: Architecture of the Alexa integration solution with Amazon Bedrock.\nThe major components are an Amazon S3 bucket, AWS Lambda, Amazon Athena, Amazon Bedrock, AWS Glue, and Amazon EventBridge.\nIn this solution, users can ask questions about data through Alexa using a skill. The Alexa Skills Kit is a software development framework you can use to create content, called skills. The solution also uses Amazon Bedrock, a fully managed service that offers several high-performance FM options from leading AI companies such as AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon, through a single API. Amazon Bedrock also has a broad set of necessary resources such as knowledge bases, agents, and guardrails to create generative AI applications with safety, privacy, and responsibility.\nThe flow of the architecture follows these steps:\nThe process starts with users asking questions in natural language through Alexa. These questions are forwarded to an AWS Lambda function. The function will interact with Anthropic\u0026rsquo;s LLM, specifically the Claude 3 Haiku model, through Amazon Bedrock API calls.\nClaude 3 Haiku receives the database structure as context, such as the description of tables and fields, through the user\u0026rsquo;s question. Based on the question and the database structure, the model generates SQL code. The generated SQL query is then executed in Amazon Athena, which retrieves data from structured \u0026ldquo;.csv\u0026rdquo; files stored in Amazon Simple Storage Service (Amazon S3). Athena uses the metadata from the data catalog managed by AWS Glue Data Catalog.\nThe data returned after executing the SQL query is used as context in a new Amazon Bedrock API call for the Claude 3 Haiku to generate a text response, explaining the query data in a natural and understandable way. This response is then returned by the Lambda function to Alexa, which converts it to audio and responds to the user.\nTo keep the solution up-to-date in case of structural changes in the database, there is a rule created in Amazon EventBridge that is triggered every time data is updated in Amazon S3 and will trigger a Lambda function that updates the data catalogs in AWS Glue by executing the AWS Glue crawler and updating the solution\u0026rsquo;s context files.\nThis solution arose from the need of the IFSP, which today has more than 50,000 students, to optimize decision-making for administrative managers in an increasingly dynamic environment that requires quick budgetary and educational decisions based on a large amount of data. IFSP worked on developing the Alexa skill in collaboration with AWS, thus enabling natural language interaction with a complex set of data present in the Nilo Peçanha Platform. This offered top management a means of accessing data and educational indicators to support decision-making with the correct alignment of the institution with its strategic and social demands.\n\u0026ldquo;Natural language interaction with the data from the Nilo Peçanha Platform is a necessary evolution to streamline strategic decision-making, based on data and innovation through the integration of AWS generative AI tools in the daily life of our institution\u0026rsquo;s managers,\u0026rdquo; said Bruno Luz, pro-rector of planning and institutional development at IFSP.\nThe data was imported and stored in a serverless data lake architecture on AWS, using Amazon S3 as the storage layer and AWS Glue and Amazon Athena for data cataloging and querying. Different prompts were tested on the Claude 3 Haiku, using techniques such as zero-shot and few-shot. The following prompt best met this use case:\n\\\\ Human: Transform this natural language question into a query for AWS Athena. Respond only with the SQL query in a single line. In the query, the table name and column names should be in quotes. ONLY the table name should have \u0026lsquo;\u0026ldquo;AwsDataCatalog\u0026rdquo;.\u0026ldquo;s3\u0026rdquo;.\u0026rsquo;. Do not generate queries related to any database alterations, only generate query questions. Assume that the database has the following tables and columns:\nTable: TaxaEvasao\nId (integer pk) Description (varchar) Quantity (number) For questions related to Dropout Rate, table TaxaEvasao, consider that the calculation of the dropout rate is the sum of the numero_de_matriculas column divided by the sum of the matriculas_numero_de_evadidos column. Additionally, the number must be in decimal format, and Athena will present a decimal if both sums have a CAST to decimal. For example, to list the dropout rates for 2022, the correct query would be:\nSELECT instituicao, sum(numero_de_matriculas) as MAT, sum(matriculas_numero_de_evadidos) as EVD, CAST ((sum(numero_de_matriculas)) AS decimal(10,2))/CAST ((sum(matriculas_numero_de_evadidos)) AS decimal(10,2)) FROM \u0026ldquo;AwsDataCatalog\u0026rdquo;.\u0026ldquo;s3\u0026rdquo;.\u0026ldquo;TaxaEvasao\u0026rdquo; WHERE \u0026ldquo;ano\u0026rdquo; = 2022 GROUP BY instituicao ORDER BY \u0026ldquo;instituicao\u0026rdquo; DESC;\nIdentify the query generated in your response and place it between the tags \\\\\nThrough this prompt and according to the user\u0026rsquo;s question, the Claude 3 Haiku responds with the SQL code. For example, for the question, \u0026ldquo;List the top 3 institutes with more student-teacher ratio for the year 2022\u0026rdquo; the result was:\n\\\\sql SELECT instituicao_nome, rap FROM \u0026ldquo;AwsDataCatalog\u0026rdquo;.\u0026ldquo;s3\u0026rdquo;.\u0026ldquo;RelacaoAlunoProfessorRAP\u0026rdquo; WHERE ano = 2021 ORDER BY rap DESC LIMIT 3; \\\\\nAfter the query is made in the data lake, the final response to the user is generated dynamically using the LLM. The following screenshot shows the returned result.\nFigure 2: Example of questions and answers in tests on the Alexa developer console.\nYou can download the source code for this solution and run your version of the Alexa skill by accessing the published example in the official AWS sample GitHub repository.\n\u0026ldquo;The use of AI with AWS allows IT areas to play an innovative role, opening doors to the use of new technologies for institutional evolution towards more prepared and agile environments at an affordable cost and compatible with the technological complexity involved,\u0026rdquo; said Leonardo Menzani Silva, director of IT at IFSP.\nSECURITY, PRIVACY, AND PROTECTION It\u0026rsquo;s important to highlight that the solution provides resource security. Data is encrypted at rest and in transit, both by Amazon Bedrock and in the integration of Alexa with the Lambda function. In this solution, we enabled data encryption in S3 buckets by default using keys that can be created and managed in the AWS Key Management Service (AWS KMS).\nAmazon Bedrock doesn\u0026rsquo;t share your data with model providers and doesn\u0026rsquo;t use it to retrain public models, meaning the service maintains data confidentiality. When you tune your FM, it is based on a private copy of that model. Amazon Bedrock complies with international standards, including ISO, SOC, CSA STAR Level 2, and GDPR, and is eligible for HIPAA.\nThe following diagram shows how access permissions to services and resources work using roles and policies created and maintained with AWS Identity and Access Management (IAM).\nFigure 3: Architectural diagram of access permissions to services and resources.\nFor the Alexa skill to be able to call a Lambda function, it needs to configure the Amazon Resource Name (ARN) of the Lambda function in the skill development console. In turn, it needs to create a resource-based policy with the ID of the skill associated with the Lambda function that allows it to be called by the skill.\nAdditionally, you need to create resource-based policies so that the Lambda function, when invoked, can access the Amazon Bedrock, Athena, and AWS Glue catalog services. Although the function does not directly access AWS Glue, the policy associated with it allows it, through Athena, to access resources stored in Amazon S3 and the data encryption key stored in AWS KMS.\nThe solution did not address data access authorization because the data is public. If you need to limit user access to data, you can use the account linking functionality of the Alexa Skills Kit.\nCOSTS This solution used Amazon Bedrock On-Demand billing, which allows the use of FMs without the need for time-based commitments. Text generation models are charged per input tokens processed and per output tokens generated.\nA token comprises a few characters and refers to the basic unit of text that a model learns to understand the prompt. For Claude, a token represents approximately 3.5 characters in English, although the exact number may vary depending on the language used. Tokens are usually hidden when interacting with language models at the \u0026ldquo;text\u0026rdquo; level, but they become relevant when examining the exact inputs and outputs of a language model.\nIn the cost estimates with IFSP, 1,000 monthly calls were stipulated. To reduce costs, the first processing, where the SQL query was generated, used the Claude 3 Haiku LLM with the input prompt using around 6,000 tokens and generating an output with around 400 tokens, costing approximately USD 1.50. In the second part, where the final response was generated, we used the Claude 3 Sonnet LLM to have a more appropriate response. The result was 400 tokens as input and around 150 tokens as output, costing approximately USD 1.20.\nFor detailed pricing information, please refer to the Amazon Bedrock Pricing page.\nCONCLUSION The integration of generative AI with Alexa for data analysis, using advanced models such as Anthropic\u0026rsquo;s Claude 3 on Amazon Bedrock, simplifies interaction with complex data, transforming natural language queries and returning accurate responses. This solution, exemplified by the success at the Federal Institute of São Paulo, demonstrates the effectiveness of the approach, making data analysis more accessible. To learn more, contact your AWS account team or the AWS Public Sector team.\nABOUT THE AUTHORS Cristiano Scandura has been in the IT industry since 1998. He joined Amazon Web Services (AWS) in 2018, where he worked on projects for enterprise clients. Currently, he specializes in generative artificial intelligence (AI), AI, and machine learning (ML) projects for AWS Worldwide Public Sector.\nGabriel Martini has experience in software engineering, solution architecture, and data science. He has worked in IT since 2014 and joined Amazon Web Services (AWS) in 2017. At AWS, he\u0026rsquo;s worked as a solutions architect for large clients and on open data research initiatives. He currently works as an artificial intelligence (AI) specialist solutions architect focusing on areas such as generative AI, machine learning operations (MLOps), and computer vision.\n"},{"uri":"https://huyle001.github.io/Hugo/en/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 1 Objectives • Connect and get acquainted with members of First Cloud Journey\n• Understand basic AWS concepts and cloud computing fundamentals\n• Get familiar with AWS Console, IAM, and basic services\nTasks to be carried out this week Day Task Start Date Completion Date Reference Material Monday - Get acquainted with FCJ members and read internship unit rules\n- Learn about AWS and Cloud Computing basics\n- Understand what AWS is and why we use it 08/09/2025 08/09/2025 Cloud Journey\nVideo 1\nVideo 2 Tuesday - Create AWS Free Tier account\n- Setup Virtual MFA Device\n- Learn about IAM basics: Users and Groups 09/09/2025 09/09/2025 Video 3\nVideo 4\nVideo 5 Wednesday - Create admin group and admin user\n- Install AWS CLI and try basic configuration\n- Practice some simple AWS CLI commands 10/09/2025 10/09/2025 Video 6\nVideo 7\nVideo 8 Thursday - Learn about EC2 basics\n- Understand what instances, AMI, and Security Groups are\n- Practice launching a simple EC2 instance 11/09/2025 11/09/2025 Video 9\nVideo 10\nVideo 11 Friday - Try connecting to EC2 via SSH\n- Learn about S3 basics\n- Create a simple S3 bucket and upload test files 12/09/2025 12/09/2025 Video 12\nVideo 13 Week 1 Achievements • Connected with First Cloud Journey members and completed bootcamp orientation.\n• Learned AWS cloud computing fundamentals, basic terminology (Regions, Services, IaaS/PaaS/SaaS), and AWS Global Infrastructure.\n• Successfully created AWS Free Tier account and configured Virtual MFA Device for security.\n• Understood IAM basics (Users, Groups, Policies) and created first admin group and IAM user.\n• Became familiar with AWS Management Console navigation and service locations.\n• Installed and configured AWS CLI with credentials (Access Key, Secret Key, Region).\n• Practiced basic AWS CLI commands: aws configure, aws s3 ls, aws iam list-users.\n• Learned EC2 fundamentals including instance types (t2.micro), AMI, and Security Groups.\n• Successfully launched first EC2 instance and practiced SSH connection.\n• Understood S3 basics (Buckets and Objects), created S3 bucket, and practiced file upload/download.\n"},{"uri":"https://huyle001.github.io/Hugo/en/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 2 Objectives • Deepen understanding of EC2 and storage services\n• Learn about VPC and networking basics in AWS\n• Get familiar with databases and other core AWS services\n• Continue practicing with AWS Console and CLI\nTasks to be carried out this week Day Task Start Date Completion Date Reference Material Monday - Review Week 1 concepts\n- Learn more about EC2 advanced features\n- Understand EBS volumes and snapshots 15/09/2025 15/09/2025 Video 14\nVideo 15\nVideo 16 Tuesday - Learn about VPC basics: Subnets, Route Tables\n- Understand Internet Gateway and NAT Gateway\n- Practice creating a simple VPC 16/09/2025 16/09/2025 Video 25\nVideo 26\nVideo 27 Wednesday - Continue learning VPC networking\n- Understand Security Groups vs NACLs\n- Practice configuring network access 17/09/2025 17/09/2025 Video 28\nVideo 29\nVideo 30 Thursday - Learn about RDS (Relational Database Service)\n- Understand database basics in AWS\n- Explore DynamoDB introduction 18/09/2025 18/09/2025 Video 35\nVideo 36\nVideo 37 Friday - Learn about Load Balancers (ELB/ALB)\n- Understand Auto Scaling basics\n- Review Week 2 concepts and practice 19/09/2025 19/09/2025 Video 40\nVideo 41\nVideo 42 Week 2 Achievements • Learned EBS (Elastic Block Store) volumes including creation, attachment, and snapshot management.\n• Understood VPC fundamentals: Subnets, CIDR blocks, Route Tables, Internet Gateway, and NAT Gateway.\n• Successfully created basic VPC setup and practiced VPC networking configuration.\n• Learned difference between Security Groups and NACLs, configured inbound/outbound rules.\n• Got introduced to RDS (Relational Database Service) and various database engines (MySQL, PostgreSQL).\n• Understood benefits of managed databases and basic DynamoDB (NoSQL) concepts.\n• Learned Load Balancers (ELB and ALB) and basic Auto Scaling concepts for high availability.\n• Improved AWS documentation reading skills and console navigation.\n"},{"uri":"https://huyle001.github.io/Hugo/en/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 3 Objectives • Practice building a simple web application on AWS\n• Learn about monitoring and logging with CloudWatch\n• Understand serverless basics with Lambda\n• Review and consolidate knowledge from Week 1 and 2\nTasks to be carried out this week Day Task Start Date Completion Date Reference Material Monday - Review VPC and networking from Week 2\n- Learn about Route 53 (DNS service)\n- Understand domain management basics 22/09/2025 22/09/2025 Video 43\nVideo 44\nVideo 45 Tuesday - Learn about CloudWatch for monitoring\n- Understand logs and metrics basics\n- Practice setting up simple alarms 23/09/2025 23/09/2025 Video 72\nVideo 73\nVideo 74 Wednesday - Introduction to AWS Lambda\n- Learn serverless computing basics\n- Try creating a simple Lambda function 24/09/2025 24/09/2025 Video 80\nVideo 81\nVideo 82 Thursday - Learn about SNS (Simple Notification Service)\n- Understand SQS (Simple Queue Service) basics\n- Get familiar with messaging services 25/09/2025 25/09/2025 Video 88\nVideo 89\nVideo 90 Friday - Practice building a simple project combining Week 1-3 services\n- Review all concepts learned so far\n- Document challenges and learnings 26/09/2025 26/09/2025 Review previous videos\nPractice hands-on Week 3 Achievements • Learned Route 53 DNS service basics and domain name management concepts.\n• Understood CloudWatch monitoring including metrics, logs, and alarms for AWS resources.\n• Practiced viewing EC2 metrics and setting up CPU usage alarms.\n• Got introduced to serverless computing and created first Lambda function (Hello World).\n• Learned SNS (Simple Notification Service) for notifications and SQS (Simple Queue Service) for message queuing.\n• Understood difference between SNS and SQS messaging patterns.\n• Practiced building small project combining EC2, S3, and networking from Week 1-3.\n• Improved troubleshooting skills and AWS documentation searching abilities.\n"},{"uri":"https://huyle001.github.io/Hugo/en/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 4 Objectives • Practice with storage services and file systems\n• Get hands-on experience with databases (RDS)\n• Learn about security monitoring and compliance basics\n• Build a small integrated project using multiple AWS services\nTasks to be carried out this week Day Task Start Date Completion Date Reference Material Monday - Review storage concepts from previous weeks\n- Learn about EFS (Elastic File System)\n- Understand differences between EBS, EFS, and S3 29/09/2025 29/09/2025 Video 17\nVideo 18\nVideo 19 Tuesday - Practice with RDS (Relational Database Service)\n- Try creating a simple MySQL/PostgreSQL database\n- Learn about database backups and snapshots 30/09/2025 30/09/2025 Video 38\nVideo 46\nVideo 47 Wednesday - Continue with SNS and SQS hands-on practice\n- Try sending notifications with SNS\n- Understand message queuing with SQS 01/10/2025 01/10/2025 Video 91\nVideo 92\nVideo 93 Thursday - Learn about CloudTrail for auditing\n- Understand AWS security best practices\n- Get familiar with basic compliance concepts 02/10/2025 02/10/2025 Video 96\nVideo 97\nVideo 98 Friday - Work on a small project integrating EC2, RDS, and S3\n- Practice connecting web server to database\n- Review and document what I\u0026rsquo;ve learned this month 03/10/2025 03/10/2025 Practice and review\nPrevious videos as reference Week 4 Achievements • Learned EFS (Elastic File System) and differences between EBS (block storage), EFS (file storage), and S3 (object storage).\n• Created first RDS database instance (MySQL), practiced database snapshots and backups.\n• Successfully connected EC2 to RDS database after troubleshooting security group configurations.\n• Practiced SNS (Simple Notification Service) for email notifications and SQS (Simple Queue Service) for message queuing.\n• Learned CloudTrail for tracking AWS API calls and security auditing purposes.\n• Built integrated project combining EC2 web server, RDS MySQL database, and S3 for static files.\n• Improved debugging skills for connection issues and security group configurations.\n• Started keeping troubleshooting journal and learned importance of organized resource naming.\n"},{"uri":"https://huyle001.github.io/Hugo/en/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 5 Objectives • Deepen understanding of S3 advanced features\n• Learn about NoSQL databases with DynamoDB\n• Practice more with serverless architecture using Lambda\n• Build a simple serverless application\nTasks to be carried out this week Day Task Start Date Completion Date Reference Material Monday - Learn about S3 versioning and object lifecycle\n- Understand S3 storage classes in detail\n- Practice with S3 advanced features 06/10/2025 06/10/2025 Video 20\nVideo 21\nVideo 22 Tuesday - Continue with S3 lifecycle policies\n- Learn about S3 Cross-Region Replication\n- Practice organizing S3 data 07/10/2025 07/10/2025 Video 23\nVideo 24 Wednesday - Introduction to DynamoDB (NoSQL database)\n- Learn about tables, items, and attributes\n- Understand primary keys and indexes\n- Try creating a simple DynamoDB table 08/10/2025 08/10/2025 Video 55\nVideo 56\nVideo 57 Thursday - Practice more with Lambda functions\n- Learn about Lambda triggers and integrations\n- Introduction to CloudFormation basics (IaC)\n- Try connecting Lambda with DynamoDB 09/10/2025 09/10/2025 Video 83\nVideo 84\nVideo 85 Friday - Build a simple serverless application\n- Use Lambda + DynamoDB + S3 together\n- Document the project and challenges 10/10/2025 10/10/2025 Practice and integration\nReview previous videos Week 5 Achievements • Learned S3 versioning, lifecycle policies, and storage classes (Standard, Intelligent-Tiering, Glacier).\n• Introduction to DynamoDB - created first NoSQL table and understood partition/sort key design.\n• Practiced Lambda functions with different triggers (S3 events, DynamoDB streams, API calls).\n• Built serverless application: S3 upload → Lambda processing → DynamoDB metadata storage.\n• Understood difference between Query vs Scan operations in DynamoDB for data retrieval.\n• Introduction to Infrastructure as Code (IaC) with CloudFormation basics and template structure.\n• Gained deeper understanding of serverless architecture patterns and event-driven workflows.\n• Improved IAM role configuration skills and learned to debug Lambda functions using CloudWatch Logs.\n• Realized serverless can be cost-effective for small applications.\n"},{"uri":"https://huyle001.github.io/Hugo/en/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 6 Objectives • Learn to expose Lambda functions via REST API with API Gateway\n• Understand API development and integration patterns\n• Practice building complete serverless applications\n• Review and consolidate knowledge from previous weeks\nTasks to be carried out this week Day Task Start Date Completion Date Reference Material Monday - Introduction to API Gateway\n- Learn about REST API concepts\n- Understand API Gateway features and use cases 13/10/2025 13/10/2025 Video 86\nVideo 87\nVideo 103 Tuesday - Create first REST API with API Gateway\n- Learn about API methods (GET, POST, PUT, DELETE)\n- Integrate API Gateway with Lambda functions 14/10/2025 14/10/2025 Video 104\nVideo 105\nVideo 106 Wednesday - Learn about API authentication and authorization\n- Understand CORS configuration\n- Practice API deployment and stages 15/10/2025 15/10/2025 Video 107\nVideo 108\nVideo 109 Thursday - Build complete serverless REST API\n- Connect API Gateway → Lambda → DynamoDB\n- Test API endpoints with tools 16/10/2025 16/10/2025 Video 110\nVideo 111 Friday - Review serverless architecture concepts\n- Optimize and document the REST API project\n- Reflect on Week 1-6 progress 17/10/2025 17/10/2025 Practice and review\nDocumentation and optimization Week 6 Achievements • Learned API Gateway fundamentals and REST API concepts (resources, methods, endpoints).\n• Created complete CRUD REST API with API Gateway integrated with Lambda functions.\n• Configured CORS for browser access and API deployment stages (dev, prod).\n• Built complete serverless stack: API Gateway → Lambda → DynamoDB for task management API.\n• Tested APIs using Postman and learned HTTP status codes and error handling.\n• Completed understanding of full serverless architecture from frontend to backend integration.\n• Understood API Gateway features including request/response transformations and API deployment stages.\n• Learned API authentication basics (API Keys, IAM) and importance of proper error handling.\n"},{"uri":"https://huyle001.github.io/Hugo/en/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 7 Objectives • Learn about containerization and Docker basics\n• Understand the difference between containers and serverless\n• Get familiar with Amazon ECS (Elastic Container Service)\n• Practice deploying containerized applications on AWS\nTasks to be carried out this week Day Task Start Date Completion Date Reference Material Monday - Introduction to containers and Docker\n- Learn what containers are and why they\u0026rsquo;re useful\n- Understand Docker basics and terminology 20/10/2025 20/10/2025 Video 112\nVideo 113\nVideo 114 Tuesday - Learn about Docker images and containers\n- Try creating a simple Dockerfile\n- Introduction to ECR (Elastic Container Registry) 21/10/2025 21/10/2025 Video 115\nVideo 116\nVideo 117 Wednesday - Introduction to Amazon ECS\n- Learn about ECS task definitions and services\n- Understand ECS clusters and container orchestration 22/10/2025 22/10/2025 Video 118\nVideo 119\nVideo 120 Thursday - Practice deploying a container to ECS\n- Learn about ECS Fargate (serverless containers)\n- Configure container networking and load balancing 23/10/2025 23/10/2025 Video 121\nVideo 122\nVideo 123 Friday - Build and deploy a simple containerized web application\n- Compare containers vs serverless architecture\n- Document learnings and use cases 24/10/2025 24/10/2025 Practice and review\nBuild container project Week 7 Achievements • Learned containerization concepts and Docker basics (images, containers, Dockerfile syntax).\n• Created first Dockerfile and built Docker images locally with proper layer management.\n• Pushed Docker images to ECR (Elastic Container Registry) after configuring authentication.\n• Introduction to ECS - understood clusters, task definitions, tasks, and services architecture.\n• Deployed containerized web application using ECS Fargate with Application Load Balancer configuration.\n• Compared containers vs serverless architecture and learned appropriate use cases for each approach.\n• Practiced creating simple containerized Node.js application from scratch to deployment.\n• Improved understanding of container orchestration and when to use containers over traditional EC2.\n"},{"uri":"https://huyle001.github.io/Hugo/en/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives • Learn about CI/CD principles and automation\n• Understand AWS developer tools for deployment pipelines\n• Practice automating application deployments\n• Review and consolidate 8 weeks of AWS learning\nTasks to be carried out this week Day Task Start Date Completion Date Reference Material Monday - Introduction to CI/CD concepts\n- Learn about CodeCommit (Git repository)\n- Understand version control in AWS 27/10/2025 27/10/2025 Video 131\nVideo 132\nVideo 133 Tuesday - Learn about CodeBuild for building applications\n- Understand CodeDeploy for deployment automation\n- Practice creating build specifications 28/10/2025 28/10/2025 Video 134\nVideo 135\nVideo 136 Wednesday - Introduction to CodePipeline\n- Learn to create automated deployment pipelines\n- Connect CodeCommit → CodeBuild → CodeDeploy 29/10/2025 29/10/2025 Video 137\nVideo 138\nVideo 139 Thursday - Build complete CI/CD pipeline\n- Practice automated deployments\n- Test pipeline with code changes 30/10/2025 30/10/2025 Video 140\nVideo 141\nVideo 142 Friday - Review AWS best practices\n- Reflect on 8 weeks of learning\n- Document complete AWS journey 31/10/2025 31/10/2025 Video 146\nVideo 147\nVideo 148 Week 8 Achievements • Learned CI/CD fundamentals including Continuous Integration and Continuous Deployment principles.\n• Practiced Git version control basics with AWS CodeCommit repositories.\n• Created buildspec.yml configuration files for CodeBuild to compile and test applications.\n• Configured CodeDeploy for automated application deployments to various targets.\n• Built complete CI/CD pipeline with CodePipeline: CodeCommit → CodeBuild → CodeDeploy automation.\n• Reduced manual deployment time from 30 minutes to 5 minutes through pipeline automation.\n• Understood benefits of automation including faster deployments, fewer errors, and consistent processes.\n• Gained practical DevOps skills and understood importance of automated testing and deployment strategies.\n"},{"uri":"https://huyle001.github.io/Hugo/en/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives • Learn about AWS workflow orchestration with Step Functions\n• Understand event-driven architecture with EventBridge\n• Practice advanced Lambda integration patterns\n• Explore container orchestration concepts\nTasks to be carried out this week Day Task Start Date Completion Date Reference Material Monday - Introduction to AWS Step Functions\n- Learn about workflow orchestration\n- Understand state machines and workflows 03/11/2025 03/11/2025 Video 48\nVideo 49\nVideo 50 Tuesday - Create Step Functions workflows\n- Practice coordinating multiple Lambda functions\n- Learn about error handling in workflows 04/11/2025 04/11/2025 Video 51\nVideo 52\nVideo 53 Wednesday - Introduction to Amazon EventBridge\n- Learn about event-driven architecture\n- Understand event routing and patterns 05/11/2025 05/11/2025 Video 75\nVideo 76\nVideo 77 Thursday - Practice EventBridge with Lambda integration\n- Create event rules and targets\n- Build event-driven workflows 06/11/2025 06/11/2025 Video 78\nVideo 79\nVideo 54 Friday - Build integrated workflow project\n- Combine Step Functions + EventBridge + Lambda\n- Review week\u0026rsquo;s learnings 07/11/2025 07/11/2025 Video 124\nVideo 125\nVideo 126 Week 9 Achievements • Learned AWS Step Functions for workflow orchestration using state machines and Amazon States Language (JSON).\n• Created workflows with different state types: Task, Choice, Wait, Parallel, Map, Pass, Succeed/Fail.\n• Implemented error handling and retry logic in Step Functions for robust workflow management.\n• Learned Amazon EventBridge for event-driven architecture including event rules, patterns, and routing.\n• Built event-driven applications routing S3 events to Lambda functions via EventBridge triggers.\n• Created integrated project combining EventBridge, Step Functions, and Lambda for complex multi-step data processing.\n• Practiced basic encryption with KMS (Key Management Service) for securing Lambda environment variables and S3 objects.\n• Learned to use Secrets Manager to store database credentials and API keys securely instead of hardcoding in Lambda.\n• Understood when to use Step Functions for long-running workflows vs direct Lambda invocation.\n• Practiced creating event patterns and scheduled events for automated task execution.\n"},{"uri":"https://huyle001.github.io/Hugo/en/3-blogstranslated/3.6-blog6/","title":"Wicked Saints Studios Integrates TikTok Within World Reborn Using AWS","tags":[],"description":"","content":" Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nWICKED SAINTS STUDIOS INTEGRATES TIKTOK WITHIN WORLD REBORN USING AWS Original Article: Wicked Saints Studios integrates TikTok within World Reborn using AWS\nOriginal Author: Matthew Nimmo\nPublication Date: 11 NOV 2025\nFaced with integrating TikTok functionality into their new mobile game, World Reborn, Wicked Saints Studios encountered a dilemma. Available market solutions were either prohibitively expensive, time-consuming to implement, or lacked TikTok integration entirely. To meet their pressing deadline, while maintaining cost efficiency, the studio developed a creative solution by building upon existing Amazon Web Services (AWS) guidance.\nTHE GAMING INDUSTRY\u0026rsquo;S TIKTOK INTEGRATION LANDSCAPE TikTok has revolutionized gaming content sharing through its short-form format. Marketing strategies are now emphasizing authentic, behind-the-scenes content, which resonates with the style of TikTok. So, studios are implementing direct Share to TikTok functionality for achievements and victories.\nWICKED SAINTS IMPLEMENTATION Wicked Saints, established in 2021 as a start-up mobile gaming company, is gearing up for their full launch of World Reborn next year. Recently they have utilized the Guidance for Custom Game Backend Hosting on AWS to kickstart their backend solution for user authentication. The solution provides a battle-tested, ready-to-go, framework that seamlessly integrated into their existing backend infrastructure.\nGuidance for Custom Game Backend Hosting on AWS provides a comprehensive framework for building scalable game backends. It offers expertise in player management, authentication, global deployment, and live service operations. The solution emphasizes building systems that can handle everything from casual mobile games to massive multiplayer experiences. It focuses on player management, scalable architecture, and global reach. Wicked Saints extended this solution to include a TikTok integration.\nCHALLENGES FACED DURING THE INTEGRATION Wicked Saints encountered several challenges during the TikTok integration process:\nDeep link encoding and state size limits: The team had to keep the state minimal and signed to work within TikTok constraints. Redirect allowlist mismatches: Ensuring synchronization between TikTok app settings and runtime hostnames required careful configuration. Extended approval times: Obtaining production API access from TikTok involved a lengthy approval process. User experience optimization: Maintaining a clean and seamless user experience, while adhering to TikTok guidelines proved challenging. ARCHITECTURE OVERVIEW Figure 1: High-level architecture.\nWicked Saints high-level architecture walkthrough:\nPlayers connect through Amazon Route 53, which passes through Amazon CloudFront and to Amazon API Gateway. To protect against malicious attacks AWS WAF is used to protect the API Gateway. From there players can refresh access tokens, login with Google, and now login with TikTok through AWS Lambda. All secrets (for example: TikTok app credentials) are stored within AWS Secrets Manager and all meta data associated with login purposes are stored within Amazon DocumentDB. Amazon Simple Storage Service (Amazon S3) hosts any of the game content that is static in nature, such as art assets. MAPPING AWS FRAMEWORK TO TIKTOK PROVIDER Wicked Saints started from the Google Play sample flow provided in the Guidance for Custom Game Backend Hosting on AWS and mirrored the pattern. Token exchange within the backend was needed for security reasons from TikTok, as they use OAuth so users can properly be authenticated with TikTok.\nWhile the other login options already had off-the-shelf means, Wicked Saints had to create their own custom login pattern for TikTok using HTTP that mirrored the existing examples within the AWS guidance. As part of this token exchange Wicked Saints needed to keep things secure by encrypting the token. This encrypted token is stored within Secrets Manager, and the encrypted key is held within Amazon DocumentDB.\nAll uses cases expressed by the existing login features offered by the standard solution guidance is mirror with this newly mapped TikTok extension. These scenarios include link to an existing account, login to existing account, and create a new account and link to the provider.\nOAUTH FLOW IMPLEMENTATION All following code sections describe how Wicked Saints made their TikTok integration possible. The code isn\u0026rsquo;t intended to be prescriptive, but to be adapted to create TikTok integration with your own mobile games.\nThe first section of code illustrates how Wicked Saints built out the token exchange. First there is a server endpoint \\GET /tiktok/login?redirectUrl={redirectUrl}\\ that is called by the client. The \\PlayerId\\ and \\IsLinking\\ are something Wicked Saints put in place to track if there is an existing user or not that is logging in through TikTok.\nThis is not a requirement with TikTok, but instead a way to see if the user calling the TikTok login endpoint is already logged in on World Reborn. If that is the case, the solution links their player account to their TikTok account. Wicked Saints receives the user id from the user claims, if they exist. The player only exists if the user is already logged into the game.\n\\\\csharp public IActionResult Login([FromQuery] string redirectUrl) { var callbackUrl = $\u0026quot;{enter your callback url here}\u0026quot;; var playerId = User.FindFirstValue(ClaimTypes.NameIdentifier); var url = tikTokService.RequestAccessToken(callbackUrl, redirectUrl, playerId); return Redirect(url); } \\\\\nWhen the redirection occurs there is a callback endpoint that is invoked. This endpoint is designed for validation. Wicked Saints validates the state, then re-redirects the client to the \\RedirectURL\\ with code. See the following example:\n\\\\csharp public IActionResult Callback([FromQuery] string code, [FromQuery] string state) { if (string.IsNullOrEmpty(code)) return BadRequest(\u0026ldquo;No code provided\u0026rdquo;); if (string.IsNullOrEmpty(state)) return BadRequest(\u0026ldquo;No state provided\u0026rdquo;); var oAuthState = AuthorizerService.GetOAuthState(state); if (oAuthState == null) return BadRequest(\u0026ldquo;Invalid state provided\u0026rdquo;); return Redirect($\u0026quot;{oAuthState.RedirectUrl}?code={code}\u0026amp;state={Uri.EscapeDataString(state)}\u0026quot;); } \\\\\nAfter a successful validation Wicked Saints exchanges the code for tokens. With another server endpoint call \\GET /tiktok/exchange?code=\u0026hellip;\u0026amp;state=\u0026hellip;\\ Wicked Saints can retrieve the access_token, refresh_token, and TikTok user ID. From there Wicked Saints proceeds exactly how it is described in the Google Play example provided in the Guidance for Custom Game Backend Hosting on AWS.\n\\\\csharp public async Task Exchange([FromQuery] string code, [FromQuery] string state) { if (string.IsNullOrEmpty(code)) return BadRequest(\u0026ldquo;No code provided\u0026rdquo;); if (string.IsNullOrEmpty(state)) return BadRequest(\u0026ldquo;No state provided\u0026rdquo;); var oAuthState = AuthorizerService.GetOAuthState(state); if (oAuthState == null) return BadRequest(\u0026ldquo;Invalid state provided\u0026rdquo;); var callbackUrl = $\u0026quot;{enter your same callback url here}\u0026quot;; var accessToken = await tikTokService.GetAccessToken(callbackUrl, code); if (accessToken == null) return BadRequest(\u0026ldquo;Failed to get access token from TikTok\u0026rdquo;); var tikTokUserId = accessToken.UserId; if (string.IsNullOrEmpty(tikTokUserId)) return BadRequest(\u0026ldquo;TikTok user ID missing from token\u0026rdquo;); // proceed with same login flow :) } \\\\\nThen Wicked Saints determines linking based on the following logic:\nIf linking is occurring and the player has a JSON Web Token (JWT); attach TikTok user id to that player (this also guards against already linked accounts). If a TikTok ID exists already then go to login; if not, then create a new player and link the accounts (game account and TikTok). Finally, Wicked Saints issues the game JWT.\nWicked Saints also built in some maintenance endpoints. One to refresh or reuse existing tokens. Another is to disconnect or revoke provider tokens and unlink the identity. Lastly, they built an end point that handles direct token hand-off flows.\nCLIENT INTEGRATION (UNITY) To keep in sync with the rest of their endpoints that are REST-based, Wicked Saints implemented a REST-first approach:\nOpen the system browser (or in-app web view) to the game backend TikTok Login endpoint. TikTok returns to a specified callback URL (TikTok callback endpoint), then the server re-redirects to the app with code and state. The client calls the TikTok exchange endpoint passing a code and state to receive the game JWT. NOTE: TikTok requires you to register redirect URIs in the app config; after consent, users are sent back to that URI. Keep your redirects on an allowlist in \\TikTok Login Kit.\nIn the future Wicked Saints will be extending the AWS Mobile SDK for Unity to support the new TikTok integration. Even though Wicked Saints went with the \\\rest api\\ integration route, you could extend the Guidance for Custom Game Backend Hosting on AWS gaming backend with AWS Mobile SDK for Unity to support your new TikTok integration.\nFor example, you could use something like the following:\n\\\\csharp public class AWSGameSDKClient : MonoBehaviour { public void LoginWithTikTok(string redirectUrl, bool mobile, Action callback) =\u0026gt; StartTikTokSignIn(redirectUrl, mobile, callback);\npublic void LinkTikTokToCurrentUser(string redirectUrl, bool mobile, Action\u0026lt;LoginRequestData\u0026gt; callback) { StartTikTokSignIn(redirectUrl, mobile, callback); } public void StartTikTokSignIn(string redirectUrl, bool mobile, Action\u0026lt;LoginRequestData\u0026gt; callback) { if (this.loginEndpoint == null) { this.LoginErrorCallback?.Invoke(\u0026quot;Login endpoint not defined\u0026quot;); return; } this.LoginCallback = callback; var url = \\$\u0026quot;{this.loginEndpoint}tiktok?redirectUrl={UnityWebRequest.EscapeURL(redirectUrl)}\u0026quot;; Application.OpenURL(url); } void OnEnable() { Application.deepLinkActivated += OnDeepLinkActivated; if (!string.IsNullOrEmpty(Application.absoluteURL)) OnDeepLinkActivated(Application.absoluteURL); } private void OnDeepLinkActivated(string url) { var uri = new System.Uri(url); var q = ParseQuery(uri.Query); if (!q.ContainsKey(\u0026quot;code\u0026quot;) || !q.ContainsKey(\u0026quot;state\u0026quot;)) return; StartCoroutine(this.CallRestApiGetForLogin( this.loginEndpoint, \u0026quot;tiktok/exchange\u0026quot;, this.SdkLoginCallback, new Dictionary\u0026lt;string,string\u0026gt; { { \u0026quot;code\u0026quot;, q[\u0026quot;code\u0026quot;] }, { \u0026quot;state\u0026quot;, q[\u0026quot;state\u0026quot;] } } )); } } \\\\\nUSING TIKTOK APIS FOR CREATOR WORKFLOWS After players successfully authenticate through the TikTok Login Kit, Wicked Saints can provide players with a way to share content directly from the game, or companion app, to their TikTok accounts. This integration requires specific permissions and follows established workflows.\nPrerequisites Before implementing TikTok content publishing, verify the following requirements are met:\nDeveloper application approval\nYour TikTok developer application must be reviewed and approved for Content Posting API access Submit necessary documentation and use cases to TikTok for review User authorization requirements\nPlayers must grant specific permissions through OAuth consent Required scopes include: \\\u000bideo.upload: For uploading video content \\\u000bideo.publish: For publishing content to TikTok Authentication implementation\nImplement TikTok OAuth v2 authentication flow Properly manage and store user access tokens Handle token refresh and expiration Content publishing workflows Option 1: Deferred publishing workflow\nWith this workflow players can upload content first and complete the posting process later in TikTok:\nInitialize upload Create an upload session with the TikTok API Obtain necessary upload tokens and endpoints Transfer video content Either upload video data directly through API endpoints Or provide a URL for TikTok to pull content (requires domain verification) User completion Player receives a TikTok notification They can then edit and complete the post within the TikTok app Provides full access to TikTok native editing features Option 2: Direct publishing workflow\nThis workflow enables immediate content posting from within your application:\nPrepare publishing interface Query \\creator_info\\ API endpoint Retrieve the user\u0026rsquo;s TikTok profile information Build appropriate publishing interface based on the user\u0026rsquo;s TikTok account type Initialize post Create new post session Set initial post parameters Prepare content metadata Export and publish Upload video content Submit post for immediate publication WHY CHOOSE AWS CUSTOM GAMING BACKEND FOR INTEGRATION Wicked Saints chose the Guidance for Custom Game Backend Hosting on AWS solution as their starting point for several key reasons:\nFlexible by design: The framework\u0026rsquo;s identity layer is provider-agnostic, allowing the quick addition of TikTok without reshaping the client or backend. Production-ready scaffolding: API Gateway with Lambda, token lifecycles, Secrets Manager, CDK stacks, and security guardrails provided proven building blocks. Scalability: The serverless, event-driven architecture, with autoscaling data stores, maintained latency budgets during growth. Built-in observability and governance: Distributed tracing, structured logs, metrics, and CDK-based tagging and cost allocation streamlined operations across development, staging, and production accounts. Faster time-to-value: AWS Quick Starts (which are automated reference architectures that streamline the deployment of key workloads on the AWS Cloud) and sample components allowed rapid setup of a working identity flow, with quick integration of .NET minimal APIs and data models. Engine-friendly: Unity samples and clean REST endpoints verify compatibility with existing Unity REST clients, with the option to extend the SDK later for TikTok helpers. Trusted security: Leveraging battle-tested patterns from AWS architects reduced the risk in a critical area. Focus on player experience: The team could concentrate on creating engaging TikTok-related features rather than building basic authentication infrastructure. Reduced development time and risk: Leveraging the framework\u0026rsquo;s extensible nature and proven security patterns accelerated the development process. SUMMARY Wicked Saints successfully integrated TikTok into their game by using the Guidance for Custom Game Backend Hosting on AWS. By leveraging this framework and adding to it, they were able to implement a robust, scalable authentication system that seamlessly incorporated the OAuth flow of TikTok. Despite facing challenges, such as deep link encoding limitations and approval processes, the team was able to focus on creating engaging player experiences rather than building authentication infrastructure from scratch.\nThe AWS solution provided a flexible, production-ready foundation that provided Wicked Saints a way to rapidly deploy their TikTok integration, while maintaining security and scalability. This case study demonstrates how game studios can leverage cloud-based solutions to streamline social media integrations and focus on core gameplay features.\nContact an AWS Representative to find out how we can help accelerate your business.\nFURTHER READING Harmony Games Deploys a Fully Custom Game Backend Utilizing AWS Cloud Development Kit (AWS CDK) Playable Worlds Scales MMO Gaming with Unity and AWS Using latency-based routing with Amazon CloudFront for a multi-Region active-active architecture ABOUT THE AUTHOR Matthew Nimmo is a Solutions Architect for Game Tech at AWS. As both a technologist and avid gamer, he helps game studios leverage cloud technology to solve complex challenges and create better player experiences.\n"},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.3-compute/5.3.1-ec2/","title":"Amazon EC2","tags":[],"description":"","content":"Initialize EC2 Instance Steps 1. Create Security Group Create a Security Group allowing HTTP (80), HTTPS (443), SSH (22), and Custom TCP (8080 - Spring Boot port). 2. Launch Instance Access EC2 Dashboard -\u0026gt; Launch Instances.\nName: Auction-Backend.\nSelect OS: Amazon Linux 2023 or Ubuntu. Select Instance Type: t3.medium (as proposed).\nSelect Key Pair (create new if not exists).\nNetwork settings: Select VPC, Public Subnet, and the created Security Group. Configure storage (default 8GB or increase if needed).\nAdvanced details:\nIAM instance profile: Select Auction-EC2-Role. Click Launch instance.\n3. Assign Elastic IP (Optional) If you want a fixed Public IP.\nGo to Elastic IPs -\u0026gt; Allocate Elastic IP address. Select the created IP -\u0026gt; Associate Elastic IP address. Select the created Instance. "},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.2-database-storage/5.2.1-rds/","title":"Amazon RDS","tags":[],"description":"","content":"Initialize Amazon RDS (MySQL) We will use Amazon RDS MySQL to store the main data of the system.\nSteps 1. Create Security Group for RDS First, we need to create a Security Group allowing connection on port 3306 from EC2. 2. Create Subnet Group Go to RDS Dashboard -\u0026gt; Subnet groups -\u0026gt; Create DB subnet group. Select the VPC and the created Private Subnets. 3. Create Database Select Databases -\u0026gt; Create database.\nSelect Standard create -\u0026gt; MySQL.\nSelect version (Engine Version). Select Free tier (if using a new account) or Dev/Test. Set DB instance identifier, Master username and password. Select Instance class (e.g., db.t3.micro).\nConfigure Storage. Configure Connectivity: Select VPC, Subnet Group, and the created Security Group. Configure authentication (Password authentication). Click Create database. "},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.4-distribution/5.4.1-ssl-certificate/","title":"Certificate Manager","tags":[],"description":"","content":"AWS Certificate Manager (ACM) To use HTTPS, we need an SSL certificate.\nSteps Access ACM Console.\nSelect Request a certificate.\nSelect Request a public certificate. Enter Domain name (e.g., *.example.com). Select DNS validation.\nClick Request.\nAfter requesting, click on the certificate ID, select Create records in Route 53 for automatic validation. Wait for status to change to Issued.\n"},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.1-preparation/5.1.1-vpc/","title":"Create VPC","tags":[],"description":"","content":"Initialize Virtual Private Cloud (VPC) The system will run inside a Virtual Private Cloud (VPC) to ensure security and resource isolation. We will create a VPC with Public Subnets (for Load Balancers) and Private Subnets (for EC2, RDS).\nSteps Access the AWS Console and search for the VPC service. Select Create VPC. Choose VPC and more configuration to quickly create a VPC along with subnets and Route Tables. Fill in the information: Name tag: Auction-VPC IPv4 CIDR block: 10.0.0.0/16 Number of Availability Zones (AZs): 2 (to ensure high availability) Number of public subnets: 2 Number of private subnets: 2 NAT gateways: None (or 1 per AZ if EC2 in private subnet needs internet access to download packages, but to save costs in this lab, you can choose None or 1). Click Create VPC. Wait for the initialization process to complete. "},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.5-deploy/5.5.2-gitlab/","title":"GitLab CI","tags":[],"description":"","content":"Configure GitLab CI Steps 1. Prepare Variables Go to Settings -\u0026gt; CI/CD -\u0026gt; Variables on the GitLab Repository. Add necessary variables:\nEC2_IP SSH_PRIVATE_KEY 2. Configuration file .gitlab-ci.yml Create a .gitlab-ci.yml file at the root of the project. The workflow includes:\nBuild: Build JAR file (Spring Boot) or Docker Image. Deploy: Copy file to EC2 and restart service. Example of a successful pipeline: "},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.1-preparation/","title":"Preparation","tags":[],"description":"","content":"Environment Preparation Before installing the main services, we need to prepare the Networking layer and Access Rights (IAM) for the resources.\nContent VPC: Create a Virtual Private Cloud to isolate the network for the system. IAM: Create necessary Roles for EC2 to access S3, Rekognition, and Textract. "},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.5-deploy/5.5.1-setup-ec2/","title":"Setup EC2 Instance Environment","tags":[],"description":"","content":"Environment Setup for EC2 Server In this section, we will assign an IAM Role to the EC2 instance and install necessary software such as Java and MariaDB to prepare for application deployment.\n1. Assign IAM role to EC2 To allow the EC2 instance to access other AWS services (e.g., Session Manager for connection without opening SSH ports, or accessing S3, RDS), we need to assign an appropriate IAM Role.\nAccess EC2 Dashboard, select the Instance you just created. Select Actions -\u0026gt; Security -\u0026gt; Modify IAM role. Select the IAM Role created in previous steps (e.g., EC2RoleForSSM) and click Update IAM role. 2. Environment Setup Connect to the EC2 Instance (using Session Manager or SSH). Then perform the following steps sequentially:\n2.1. Update System Run the following command to update to the latest software packages:\nsudo dnf update -y 2.2. Install Java Our application runs on the Java platform, so installing the Java Development Kit (JDK) is required. Here we use Amazon Corretto 21 (headless version is more suitable for command-line interfaces without graphics).\nsudo dnf install java-21-amazon-corretto-headless -y Check Java version after installation:\njava -version 2.3. Install MariaDB Client and Initialize Database Install MariaDB client to connect and interact with RDS.\nsudo dnf install mariadb105 -y Connect to the created RDS database instance. Replace \u0026lt;rds-endpoint\u0026gt;, \u0026lt;username\u0026gt; with your actual information:\nmysql -h \u0026lt;rds-endpoint\u0026gt; -u \u0026lt;username\u0026gt; -p After successfully entering the password, create the database for the application:\nCREATE DATABASE tickets; SHOW DATABASES; 2.4. Setup Service for Auto-starting Java Springboot Application Run the following command to create a service file\nsudo nano /etc/systemd/system/\u0026lt;service-name\u0026gt;.service Enter the service file content, configure environment variables for the application\nUse key combination Ctrl + O, Enter and Ctrl + X to save and exit.\nUse the following commands to apply changes\nsudo systemctl daemon-reload sudo systemctl restart \u0026lt;service-name\u0026gt; Check status using command status Use command enable so the service automatically runs every time the EC2 instance starts\nsudo systemctl enable \u0026lt;service-name\u0026gt; Command to set timezone to synchronize (in case Java application needs to match Vietnam time)\nsudo timedatectl set-timezone Asia/Ho_Chi_Minh Check result with command\ndate "},{"uri":"https://huyle001.github.io/Hugo/en/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":" Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDEVOPS ON AWS WORKSHOP Event Information Date \u0026amp; Time: Monday, November 17, 2025, 8:30 AM – 5:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent Purpose The workshop was designed to provide comprehensive knowledge and hands-on experience with AWS DevOps services, covering CI/CD pipelines, Infrastructure as Code, container services, and monitoring \u0026amp; observability. The event aimed to help participants understand DevOps culture, principles, and best practices while exploring practical implementation of DevOps workflows on AWS.\nAgenda Overview Morning Session (8:30 AM – 12:00 PM) 8:30 – 9:00 AM | Welcome \u0026amp; DevOps Mindset\nRecap of AI/ML session from previous workshop DevOps Culture and Principles: Understanding the cultural shift from traditional IT to DevOps, emphasizing collaboration, automation, and continuous improvement Benefits and Key Metrics: DORA Metrics: Deployment frequency, lead time for changes, mean time to recovery (MTTR), change failure rate MTTR (Mean Time To Recovery): Measuring how quickly teams can recover from failures Deployment Frequency: Tracking how often teams deploy code to production Discussion on how DevOps practices improve software delivery and operational performance 9:00 – 10:30 AM | AWS DevOps Services – CI/CD Pipeline\nSource Control: AWS CodeCommit, Git Strategies\nAWS CodeCommit: Fully managed source control service, secure Git repositories Git Strategies: GitFlow: Feature branches, develop, release branches workflow Trunk-based Development: Main branch focused, short-lived feature branches Best practices for branching strategies based on team size and project requirements Build \u0026amp; Test: CodeBuild Configuration, Testing Pipelines\nAWS CodeBuild: Fully managed build service that compiles source code, runs tests, and produces ready-to-deploy software packages Build Configuration: Buildspec files, environment variables, and build artifacts Testing Pipelines: Unit tests, integration tests, and automated test execution Integration with testing frameworks and code quality tools Deployment: CodeDeploy with Blue/Green, Canary, and Rolling Updates\nAWS CodeDeploy: Automated application deployments to EC2, Lambda, or on-premises servers Blue/Green Deployment: Zero-downtime deployment by running two identical production environments Canary Deployment: Gradual rollout to a small percentage of users before full deployment Rolling Updates: Incremental deployment across instances with automatic rollback capabilities Choosing the right deployment strategy based on application requirements Orchestration: CodePipeline Automation\nAWS CodePipeline: Fully managed continuous delivery service for automating release pipelines Pipeline Stages: Source, Build, Test, Deploy, and Approval stages Integration: Connecting CodeCommit, CodeBuild, CodeDeploy, and other AWS services Automation: Automated triggers, parallel execution, and pipeline visualization Demo: Full CI/CD Pipeline Walkthrough\nThe demonstration showcased a complete CI/CD pipeline:\nSetting up CodeCommit repository Configuring CodeBuild for automated builds and tests Creating CodeDeploy application with Blue/Green deployment Building CodePipeline to orchestrate the entire workflow Testing the pipeline with code changes and observing automated deployment 10:30 – 10:45 AM | Break\nNetworking and refreshments.\n10:45 AM – 12:00 PM | Infrastructure as Code (IaC)\nAWS CloudFormation: Templates, Stacks, and Drift Detection\nCloudFormation Templates: JSON/YAML templates for defining AWS resources Stacks: Collections of AWS resources managed as a single unit Drift Detection: Identifying changes made outside of CloudFormation Stack Updates: Updating infrastructure with change sets and rollback capabilities Best Practices: Template organization, parameterization, and nested stacks AWS CDK (Cloud Development Kit): Constructs, Reusable Patterns, and Language Support\nAWS CDK: Define cloud infrastructure using familiar programming languages (TypeScript, Python, Java, C#, Go) Constructs: Reusable cloud components, from low-level resources to high-level patterns Reusable Patterns: Pre-built solutions for common use cases (VPC, ECS clusters, serverless applications) Language Support: TypeScript, Python, Java, C#, Go, and JavaScript Benefits: Type safety, IDE support, and easier testing compared to CloudFormation templates Demo: Deploying with CloudFormation and CDK\nThe demonstration compared both approaches:\nCloudFormation: Deploying a VPC and EC2 instance using YAML template CDK: Same infrastructure using TypeScript with CDK constructs Highlighting the differences in approach, maintainability, and developer experience Discussion: Choosing between IaC Tools\nWhen to use CloudFormation vs CDK Considerations: team expertise, project complexity, and maintenance requirements Hybrid approaches: Using both tools together for different parts of infrastructure Lunch Break (12:00 – 1:00 PM)\nSelf-arranged lunch break.\nAfternoon Session (1:00 – 5:00 PM) 1:00 – 2:30 PM | Container Services on AWS\nDocker Fundamentals: Microservices and Containerization\nContainerization Concepts: Understanding containers, images, and containerization benefits Microservices Architecture: Breaking monolithic applications into smaller, independent services Docker Basics: Dockerfile, image building, container lifecycle Benefits: Portability, consistency, and resource efficiency Amazon ECR: Image Storage, Scanning, Lifecycle Policies\nAmazon ECR: Fully managed Docker container registry Image Storage: Secure, scalable storage for Docker images Image Scanning: Automated vulnerability scanning for container images Lifecycle Policies: Automating image cleanup and retention policies Integration: Seamless integration with ECS, EKS, and other AWS services Amazon ECS \u0026amp; EKS: Deployment Strategies, Scaling, and Orchestration\nAmazon ECS: Fully managed container orchestration service Task Definitions: Container specifications, resource requirements, and networking Services: Long-running tasks with load balancing and auto-scaling Deployment Strategies: Rolling updates, Blue/Green deployments Scaling: Auto-scaling based on CPU, memory, or custom metrics Amazon EKS: Managed Kubernetes service Kubernetes Concepts: Pods, Services, Deployments, and Namespaces EKS Features: Managed control plane, node groups, and add-ons Deployment Strategies: Rolling updates, Canary deployments with Istio/App Mesh Scaling: Cluster autoscaler and horizontal pod autoscaler AWS App Runner: Simplified Container Deployment\nApp Runner: Fully managed service for building and running containerized applications Simplified Deployment: Deploy from source code or container image Auto-scaling: Automatic scaling based on traffic Use Cases: Web applications, APIs, and microservices Comparison: When to use App Runner vs ECS vs EKS Demo \u0026amp; Case Study: Microservices Deployment Comparison\nThe demonstration compared different container deployment options:\nDeploying a simple web application with App Runner Same application with ECS Fargate Comparison of setup complexity, cost, and operational overhead Case study: Choosing the right container service for different scenarios 2:30 – 2:45 PM | Break\nNetworking and refreshments.\n2:45 – 4:00 PM | Monitoring \u0026amp; Observability\nCloudWatch: Metrics, Logs, Alarms, and Dashboards\nCloudWatch Metrics: Collecting and tracking metrics from AWS services and custom applications CloudWatch Logs: Centralized logging, log groups, and log streams CloudWatch Alarms: Automated actions based on metric thresholds CloudWatch Dashboards: Customizable dashboards for visualizing metrics and logs Best Practices: Metric naming conventions, log retention, and alarm configuration AWS X-Ray: Distributed Tracing and Performance Insights\nAWS X-Ray: Service for analyzing and debugging distributed applications Distributed Tracing: End-to-end request tracing across microservices Service Map: Visual representation of application architecture and dependencies Performance Insights: Identifying bottlenecks and performance issues Integration: X-Ray SDK integration with applications and AWS services Demo: Full-Stack Observability Setup\nThe demonstration showed:\nSetting up CloudWatch metrics and logs for an application Creating CloudWatch dashboards for monitoring Configuring CloudWatch alarms for alerting Enabling X-Ray tracing for distributed tracing Viewing service maps and trace analysis Best Practices: Alerting, Dashboards, and On-Call Processes\nAlerting Strategy: Setting up meaningful alerts, avoiding alert fatigue Dashboard Design: Creating effective dashboards for different audiences (developers, operations, management) On-Call Processes: Incident response procedures, escalation paths, and runbooks SLO/SLI: Service Level Objectives and Indicators for measuring reliability 4:00 – 4:45 PM | DevOps Best Practices \u0026amp; Case Studies\nDeployment Strategies: Feature Flags, A/B Testing\nFeature Flags: Gradual feature rollouts, canary releases, and instant rollbacks A/B Testing: Comparing different versions to optimize user experience Tools: AWS AppConfig, LaunchDarkly integration Best Practices: Feature flag management and testing strategies Automated Testing and CI/CD Integration\nTesting Pyramid: Unit tests, integration tests, and end-to-end tests Test Automation: Automated test execution in CI/CD pipelines Quality Gates: Blocking deployments based on test results Test Coverage: Measuring and improving test coverage Incident Management and Postmortems\nIncident Response: Detection, response, and recovery procedures Postmortems: Learning from incidents, documenting root causes Blameless Culture: Focusing on system improvements rather than individual blame Tools: Incident management tools and communication channels Case Studies: Startups and Enterprise DevOps Transformations\nStartup Case Study: Rapid scaling with DevOps practices, cost optimization Enterprise Case Study: Large-scale migration to DevOps, cultural transformation Lessons Learned: Common challenges and solutions ROI: Measuring the impact of DevOps adoption 4:45 – 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up\nDevOps Career Pathways: Career progression in DevOps, required skills AWS Certification Roadmap: Relevant AWS certifications for DevOps engineers AWS Certified DevOps Engineer – Professional AWS Certified Solutions Architect AWS Certified SysOps Administrator Next Steps: Resources for continued learning and practice Closing Remarks: Summary of key takeaways and action items Key Highlights CI/CD Pipeline: AWS CodePipeline provides a complete solution for automating software delivery from source to production Infrastructure as Code: Both CloudFormation and CDK offer powerful ways to manage infrastructure, with CDK providing better developer experience Container Services: AWS offers multiple container options (ECS, EKS, App Runner) for different use cases and complexity levels Observability: CloudWatch and X-Ray together provide comprehensive monitoring and tracing capabilities DevOps Culture: Success requires cultural change, not just tools and technology Best Practices: Feature flags, automated testing, and incident management are essential for modern DevOps Key Learnings DevOps is Culture First: Tools are important, but cultural transformation is the foundation of DevOps success CI/CD Automation: Automating the entire software delivery pipeline significantly improves speed and reliability IaC Benefits: Infrastructure as Code enables version control, repeatability, and faster infrastructure changes Container Strategy: Choosing the right container service depends on complexity, team expertise, and operational requirements Observability is Critical: Comprehensive monitoring and tracing are essential for maintaining reliable systems Continuous Improvement: DevOps is about continuous learning and improvement, not a one-time implementation Application to My Work Implement CI/CD: Set up CodePipeline for automated deployments in current projects Adopt IaC: Start using CloudFormation or CDK for infrastructure management Container Migration: Evaluate containerization opportunities for existing applications Improve Monitoring: Enhance CloudWatch dashboards and alarms for better visibility Practice DevOps: Apply DevOps principles and practices in daily work Incident Management: Establish incident response procedures and postmortem practices Personal Experience This full-day DevOps workshop was comprehensive and highly practical:\nThe CI/CD pipeline demonstration was particularly valuable, showing how to automate the entire software delivery process Learning about different IaC tools helped me understand when to use CloudFormation vs CDK The container services comparison provided clear guidance on choosing the right service for different scenarios The observability session emphasized the importance of monitoring and tracing for maintaining reliable systems The case studies provided real-world insights into DevOps transformations The career pathway discussion was motivating and provided clear direction for professional development Takeaways Start Small: Begin with basic CI/CD automation and gradually expand DevOps practices Culture Matters: DevOps success requires team collaboration and cultural change Choose the Right Tools: Select tools based on team expertise and project requirements Monitor Everything: Comprehensive observability is essential for reliable systems Learn Continuously: DevOps practices evolve rapidly, requiring continuous learning Measure Success: Use DORA metrics to track DevOps improvements and ROI Event Photos "},{"uri":"https://huyle001.github.io/Hugo/en/3-blogstranslated/3.2-blog2/","title":"Making SaaS Products Accessible in AWS Marketplace","tags":[],"description":"","content":" Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nMAKING SAAS PRODUCTS ACCESSIBLE IN AWS MARKETPLACE Original Article: Making SaaS products accessible in AWS Marketplace\nOriginal Authors: Len Gomes and Ricardo Oriol\nPublication Date: 04 JUN 2025\nAWS Marketplace sellers offering software-as-a-service (SaaS) solutions frequently seek guidance on understanding the options and best practices for granting buyers access to their software solutions. To address these questions, we explain the three main available access options that can help sellers effectively design and build access mechanisms to their SaaS solutions listed in AWS Marketplace.\nWhen AWS Marketplace sellers list SaaS products in AWS Marketplace, they need to make several key decisions that affect how buyers interact with their solutions. These decisions go beyond pricing and billing integration to include how customers will access their product. This post explores the three main access options for SaaS solutions in AWS Marketplace to help sellers make an informed decision that aligns with their product\u0026rsquo;s architecture and customer needs. Sellers have the flexibility to implement any combination of these access options: They can choose to support one, two, or all three access modes.\nUSE THE SELLER\u0026rsquo;S WEBSITE WITH AWS MARKETPLACE SERVERLESS INTEGRATION When using the seller\u0026rsquo;s website as the main access point for an AWS Marketplace software as a service (SaaS) offering, the integration process can be streamlined with the AWS Marketplace serverless SaaS integration solution. This approach lets the seller keep the existing website infrastructurewhere customers register, sign in, and get supportwhile taking advantage of AWS Marketplace automated deployment capabilities.\nThe AWS Quick Start deployment sets up core functions including customer registration handling, access management, entitlement updates, and usage metering. The seller maintains full control over their customer experience and user interface. The serverless integration architecture reduces the development work needed for AWS Marketplace integration by handling much of the infrastructure setup through automated workflows. While the seller\u0026rsquo;s website remains the public endpoint for customer interactions, the serverless backend manages the AWS Marketplace connection points, security implementations, and subscription lifecycle events. This approach combines the control of the existing web presence with AWS serverless architecture, helping the seller get to market faster while maintaining direct customer relationships.\nWORKSHOP RESOURCES A workshop is available to help sellers implement Quick Start at Integrate your SaaS with the Serverless SaaS Integration reference. This hands-on workshop guides you through:\nDeploying the serverless integration solution Configuring Amazon Simple Notification Service (Amazon SNS) notifications Setting up registration URLs Testing buyer subscription flow Managing entitlements and metering USE QUICK LAUNCH Quick Launch changes complex manual configuration into a streamlined, automated experience, changing how sellers make their SaaS products accessible to buyers in AWS Marketplace. Quick Launch uses preconfigured AWS CloudFormation templates validated by both the software provider and AWS to provide secure and standardized deployments while reducing technical complexity for customers. The automation handles tasks such as AWS Identity and Access Management (IAM) role configuration, security group management, and cross-account permissions, which means customers can focus on using the product rather than managing infrastructure.\nHOW IT WORKS To implement Quick Launch successfully, sellers must have a seller account and a separate test buyer account to properly validate the Quick Launch experience. The seller\u0026rsquo;s SaaS product must be listed in AWS Marketplace with Limited or Public visibility status to enable Quick Launch configuration.\nIMPLEMENTATION STEPS To configure Quick Launch the seller must:\nOpen the product in the AWS Marketplace Management Portal and choose the Fulfillment options tab Enable Quick Launch configuration and enter the sign-in or registration URL Create an AWS CloudFormation template following AWS Well-Architected Framework principles Configure AWS CloudFormation template details such as title, description, Amazon Simple Storage Service (Amazon S3) URL for the template, and required IAM permissions to deploy the template. Refer to IAM policies required for the seller account for the full list of policies. (Optional) Add instructions and documentation links for manual configuration Specify the product access URL for post-deployment to launch the product (Optional) Provide a comma-separated list of AWS allowlisted accounts to test with Limited visibility Submit for AWS Marketplace review Update visibility to Public after approval WORKSHOP RESOURCES A workshop is available to help sellers implement Quick Launch: Enable SaaS Quick Launch. This hands-on workshop guides sellers through:\nTemplate creation Security configuration Testing procedures Template development best practices Deployment validation USE AWS PRIVATELINK AWS sellers can deliver their SaaS through Amazon Virtual Private Cloud (Amazon VPC) using AWS PrivateLink to configure their products as VPC endpoint services. AWS Marketplace supports AWS PrivateLink, a highly available and scalable AWS technology that allows sellers to use the Amazon network to provide buyers access to SaaS products in a provider and consumer model. Through a VPC endpoint, a private connection between the buyer\u0026rsquo;s VPC and the seller\u0026rsquo;s product is created without requiring access over the internet or through a NAT device, a virtual private network (VPN) connection, or AWS Direct Connect. This connection method increases security to buyers because they access the seller\u0026rsquo;s service through the Amazon private network rather than through the internet.\nThe following diagram is the architecture for the solution using AWS PrivateLink.\nFigure 1: Independent software vendor (ISV) Account as provider and Customer Account as consumer\nAn interface endpoint is the entry point for traffic destined to a PrivateLink powered service such as an AWS Marketplace product. PrivateLink deploys elastic network interfaces in the client VPC with unique IP addresses from the client\u0026rsquo;s subnet. This architectural design provides complete network isolation and eliminates IP overlapping by default. PrivateLink can thus be used to bring a service into a VPC. Security groups can be associated with these endpoint network interfaces to increase security. AWS PrivateLink connects the interface endpoint to a Network Load Balancer used as a front-end in the provider\u0026rsquo;s VPC.\nIMPLEMENTATION STEPS FOR SELLERS To configure a SaaS product to be available through an Amazon VPC endpoint sellers must:\nHave a SaaS product listing in Limited or Public visibility.\nRequest a certificate from AWS Certificate Manager (ACM) for a user-friendly DNS name.\nBefore ACM issues a certificate, it validates that the seller owns or controls the domain names in the certificate request. AWS recommends that sellers provide a private DNS name that buyers can use when they create their VPC endpoints. Create a Network Load Balancer in the region where the product is deployed. AWS recommends multiple availability zones.\nCreate a VPC endpoint service and associate the Network Load Balancer to the endpoint service.\nVerify that you can access the service through the Network Load Balancer.\nSubmit your PrivateLink enabled product to the AWS Marketplace Seller Operations team by emailing the following information:\nThe endpoint and the AWS account used to create it. The endpoint looks similar to this: \\com.amazonaws.vpce.us-east-1.vpce-svc-0daa010345a21646\\ The user-friendly DNS name. This is the DNS name that AWS Marketplace buyers use to access the seller\u0026rsquo;s product. The AWS account used to request certificates and the private DNS name that buyers will use to access the VPC endpoint. Delegate the subdomain of the user-friendly DNS name, such as \\\u0007pi.vpce.example.com, to the name servers provided by the AWS Marketplace Seller Operations team. In the seller\u0026rsquo;s DNS system, they must create a name server (NS) resource record to point this subdomain to the Amazon Route 53 name servers provided by the AWS Marketplace Seller Operations team so that DNS names (such as \\\u000bpce-0ac6c347a78c90f8.api.vpce.example.com) are publicly resolvable.\nThe AWS Marketplace Seller Operations team verifies the seller\u0026rsquo;s company\u0026rsquo;s identity and the DNS name used for the service being registered (such as \\\u0007pi.vpce.example.com). After verification, the DNS name overrides the default base endpoint DNS name.\nWhen AWS Marketplace buyers subscribe to a PrivateLink enabled product and create a VPC endpoint, the service is shown under Your AWS Marketplace Services in AWS Console. The AWS Marketplace Seller Operations team uses the user-friendly DNS name for ease of discovery of the service when creating the VPC endpoint. The endpoints are shown as network interfaces in the subnets. Local IP addresses and Region and zonal DNS names are assigned to the endpoints.\nWith the launch of native cross-Region connectivity for AWS PrivateLink, service providers can now share and create access through VPC endpoint services across different Regions. This helps service providers offer SaaS solutions privately to a global audience from a single Region. Consumers can use interface endpoints to connect to cross-Region enabled services, the same as they would to services in the same Region. Cross-Region connectivity over PrivateLink is simple, secure, and customizable to fit a variety of use cases. In this Networking \u0026amp; Content Delivery Blog post released in December 2024, Introducing Cross-Region Connectivity for AWS PrivateLink, my colleagues introduce cross-Region connectivity for AWS PrivateLink, including steps to set it up.\nCONCLUSION AWS Marketplace offers SaaS sellers three primary flexible options for granting buyers access to their solutions: Whether choosing the flexibility of a seller\u0026rsquo;s website, the streamlined automation of Quick Launch, or the enhanced security of AWS PrivateLink, sellers can strategically design their product access mechanism to optimize customer experience and marketplace integration. By carefully evaluating their product\u0026rsquo;s technical requirements, security considerations, and growth objectives, SaaS providers can take advantage of multiple access options concurrently or start with a single method to effectively expand their reach, streamline customer onboarding, and create seamless software delivery experiences within AWS Marketplace.\nFor more information, refer to the AWS Marketplace Seller Guide documentation.\nABOUT THE AUTHORS Len Gomes is a Partner Solutions Architect working with top software companies (ISV\u0026rsquo;s) that partner with AWS across EMEA. He helps partners architect, optimize, and deliver cloud-based solutions that drive business value for customers while ensuring technical excellence and AWS best practices. Len has a passion for networking, hybrid and edge services and solutions. In his free time, he enjoys playing and watching football, outdoor activities, and barbecuing. He is skilled at grilling and considers himself a barbecue pit-master.\nRicardo Oriol is a Partner Solutions Architect at AWS, where he works with Partners across EMEA to deliver impactful, scalable solutions for Customers. In his role, Ricardo helps Partners discover and develop innovative cloud capabilities that drive business transformation. With a passion for cloud architecture and enabling growth through AWS, Ricardo is dedicated to empowering Customers through Partners at scale. Outside of work, he enjoys exploring emerging technologies, reading, and staying active through outdoor sports.\n"},{"uri":"https://huyle001.github.io/Hugo/en/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives • Review and consolidate AWS knowledge from 10 weeks\n• Learn about monitoring and observability best practices\n• Understand cost optimization fundamentals\n• Explore AWS security and compliance basics\n• Prepare final project and documentation\nTasks to be carried out this week Day Task Start Date Completion Date Reference Material Monday - Review core AWS services (EC2, VPC, S3)\n- Advanced CloudWatch monitoring\n- Learn about CloudWatch dashboards and alarms 10/11/2025 10/11/2025 Video 31\nVideo 32\nVideo 33 Tuesday - Cost optimization basics\n- AWS pricing models and cost monitoring\n- Best practices for reducing AWS costs 11/11/2025 11/11/2025 Video 94\nVideo 95\nVideo 99 Wednesday - AWS security best practices\n- Compliance and governance basics\n- Well-Architected Framework introduction 12/11/2025 12/11/2025 Video 100\nVideo 101\nVideo 102 Thursday - Review serverless and container architectures\n- CI/CD pipeline best practices\n- Prepare for final project 13/11/2025 13/11/2025 Video 143\nVideo 144\nVideo 145 Friday - Final review and reflection\n- Document complete learning journey\n- Plan next steps for continued learning 14/11/2025 14/11/2025 Video 34\nVideo 149\nReview and documentation Week 10 Achievements • Reviewed all major AWS services from 10 weeks across compute, storage, database, networking, and integration categories.\n• Created CloudWatch dashboards, alarms, and metrics filters for proactive monitoring and alerting.\n• Learned cost optimization strategies: right-sizing instances, spot instances, reserved instances, and S3 lifecycle policies.\n• Studied AWS security best practices: IAM least privilege, encryption (at rest and in transit), network security, and CloudTrail auditing.\n• Learned about AWS KMS (Key Management Service) for encryption key management and Secrets Manager for secure credential storage.\n• Introduction to AWS Well-Architected Framework\u0026rsquo;s 5 pillars: Operational Excellence, Security, Reliability, Performance Efficiency, and Cost Optimization.\n• Reviewed architecture patterns learned: 3-tier architecture, serverless, event-driven, containerized, and CI/CD pipeline implementations.\n• Solidified understanding of Serverless \u0026amp; Modern Apps track chosen from Week 5-6 (Lambda, API Gateway, DynamoDB, Step Functions).\n• Understood shared responsibility model: AWS secures infrastructure, customers secure applications and data.\n• Practiced cost monitoring with AWS Cost Explorer and learned importance of budgets and alerts.\n"},{"uri":"https://huyle001.github.io/Hugo/en/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives • Build hands-on projects using services learned over 10 weeks\n• Practice combining multiple AWS services in real applications\n• Strengthen understanding through practical implementation\n• Create portfolio-ready projects with documentation\nTasks to be carried out this week Day Task Start Date Completion Date Reference Material Monday - Design and plan Project 1: Simple Blog Application\n- Set up architecture (EC2 + RDS + S3)\n- Create VPC and security configurations 17/11/2025 17/11/2025 Video 39\nVideo 127\nVideo 128 Tuesday - Implement Project 1 backend and database\n- Deploy application on EC2\n- Configure RDS for blog posts storage 18/11/2025 18/11/2025 Video 129\nVideo 130\nReview Week 2-4 materials Wednesday - Design and build Project 2: Serverless To-Do API\n- Create API Gateway + Lambda + DynamoDB\n- Implement CRUD operations 19/11/2025 19/11/2025 Video 58\nVideo 59\nVideo 60 Thursday - Build Project 3: Automated Image Processing\n- S3 upload triggers Lambda\n- Process images and store metadata in DynamoDB 20/11/2025 20/11/2025 Video 61\nVideo 62\nVideo 63 Friday - Create CI/CD pipeline for one project\n- Document all projects with architecture diagrams\n- Clean up resources and review week 21/11/2025 21/11/2025 Video 64\nVideo 65\nReview Week 8 CI/CD notes Week 11 Achievements • Built Project 1 (Blog Application): 3-tier architecture with S3 static frontend, EC2 Node.js backend, RDS MySQL database, and Application Load Balancer.\n• Built Project 2 (Serverless To-Do API): Complete CRUD operations using API Gateway, 5 Lambda functions, and DynamoDB with API key authentication.\n• Built Project 3 (Automated Image Processing): S3 upload trigger, Lambda with Pillow library for thumbnail generation, DynamoDB for metadata storage.\n• Implemented CI/CD pipeline for To-Do API with CodeCommit, CodeBuild, and automated Lambda deployment.\n• Created comprehensive documentation with architecture diagrams, setup instructions, API documentation, and cost estimates for all projects.\n• Gained practical experience integrating multiple AWS services and troubleshooting distributed systems in real-world scenarios.\n• Applied all knowledge from Week 1-10 into working projects that demonstrate AWS skills for portfolio.\n• Learned importance of testing each component separately before integration to reduce debugging time.\n"},{"uri":"https://huyle001.github.io/Hugo/en/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives • Review all AWS services and concepts from 12 weeks\n• Prepare for AWS Certified Cloud Practitioner exam\n• Practice exam questions and identify knowledge gaps\n• Document complete learning journey\n• Plan career next steps with AWS skills\nTasks to be carried out this week Day Task Start Date Completion Date Reference Material Monday - Review Week 1-4: Core Services (IAM, EC2, S3, VPC, RDS)\n- Create study notes and flashcards\n- Practice questions on core services 24/11/2025 24/11/2025 Video 150\nVideo 151\nReview Week 1-4 worklogs Tuesday - Review Week 5-8: Advanced Services (Lambda, DynamoDB, API Gateway, Containers, CI/CD)\n- Create concept maps\n- Practice questions on advanced topics 25/11/2025 25/11/2025 Video 152\nVideo 153\nReview Week 5-8 worklogs Wednesday - Review Week 9-10: Integration, Monitoring, Cost, Security\n- Study AWS Well-Architected Framework\n- Practice questions on best practices 26/11/2025 26/11/2025 Video 154\nVideo 155\nWell-Architected whitepaper Thursday - Full practice exam (Cloud Practitioner level)\n- Review incorrect answers and fill knowledge gaps\n- Focus on weak areas 27/11/2025 27/11/2025 Video 156\nVideo 157\nAWS practice exams Friday - Final review and reflection\n- Document complete 12-week journey\n- Create career action plan\n- Celebrate completion! 28/11/2025 28/11/2025 Video 158\nVideo 159\nComplete journey review Week 12 Achievements • Conducted comprehensive review of all AWS services learned across 12 weeks with self-assessment and confidence ratings.\n• Created 200+ study flashcards, service comparison charts, and concept maps for AWS Cloud Practitioner exam preparation.\n• Studied AWS Cloud Practitioner exam topics including Support Plans, AWS Organizations, Billing and Pricing, and Global Infrastructure.\n• Completed multiple practice exams with progressive improvement from 65% to 85% success rate.\n• Documented complete 12-week journey showing skills progression from 1/10 to 7.5/10 overall AWS confidence level.\n• Created career action plan including AWS Cloud Practitioner certification pathway and AWS Solutions Architect Associate roadmap.\n• Built job-ready portfolio with 3 complete projects, GitHub repository, professional documentation, and architecture diagrams.\n• Shared learning experience through blog post documenting Week 11 projects and key AWS lessons learned over 12 weeks.\n• Key takeaways: Hands-on practice is irreplaceable, IAM permissions solve 80% of errors, cost awareness from day one is critical.\n"},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.2-database-storage/5.2.2-elasticache/","title":"Amazon ElastiCache","tags":[],"description":"","content":"Initialize Amazon ElastiCache (Redis) Redis helps cache frequent queries and store user sessions.\nSteps 1. Create Subnet Group Go to ElastiCache Dashboard -\u0026gt; Subnet groups -\u0026gt; Create subnet group. 2. Create Security Group Create a Security Group allowing port 6379 from EC2. 3. Create Redis Cluster Select Redis OSS caches -\u0026gt; Create cache.\nSelect Configure and create a new cluster.\nSelect Cluster mode disabled (for simplicity and cost savings). Configure Redis info. Configure Node type, e.g., cache.t3.micro. Select Subnet group. Select Security Group. Click Create.\n"},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.4-distribution/5.4.2-alb/","title":"Application Load Balancer","tags":[],"description":"","content":"Application Load Balancer (ALB) ALB will distribute traffic to EC2 instances and handle SSL termination.\nSteps 1. Create Security Group for ALB Allow HTTP (80) and HTTPS (443) from 0.0.0.0/0. 2. Create Target Group Create a Target Group of type Instances. Protocol HTTP/8080 (Backend port). Register EC2 instances into the Target Group. 3. Create Load Balancer Go to Load Balancers -\u0026gt; Create load balancer.\nSelect Application Load Balancer.\nSelect VPC and Public Subnets.\nSelect the created Security Group.\nConfigure Listeners:\nHTTP:80 -\u0026gt; Redirect to HTTPS (recommended). HTTPS:443 -\u0026gt; Forward to Target Group. In the HTTPS listener section, select the created ACM Certificate. Click Create load balancer.\nAfter creating the Load Balancer, you can reconfigure the security group for the EC2 instance to only accept inbound rules from the Load Balancer. "},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.1-preparation/5.1.2-iam/","title":"Create IAM Role","tags":[],"description":"","content":"Create IAM Role for EC2 EC2 needs access to S3 to retrieve code/images, and permissions to call Rekognition and Textract APIs. Instead of storing Access Keys in the code, we will use an IAM Role attached to the EC2 instance.\nSteps Access IAM Dashboard. Select Roles -\u0026gt; Create role. In the Trusted entity type step, select AWS service. In Use case, select EC2. Click Next. In the Add permissions step, search and select the following Policies: AmazonS3FullAccess (Or a policy limited to the project bucket only). AmazonRekognitionFullAccess. AmazonTextractFullAccess. AmazonSSMManagedInstanceCore (To remote into EC2 via Session Manager if needed). Click Next. Name the Role Auction-EC2-Role. Review and click Create role. "},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.2-database-storage/","title":"Database &amp; Storage","tags":[],"description":"","content":"Database and Storage Setup In this section, we will initialize:\nAmazon RDS: Relational Database (MySQL). Amazon ElastiCache: Redis cache. Amazon S3: Object storage (images, source code). These services will serve as the backend data storage for the application.\n"},{"uri":"https://huyle001.github.io/Hugo/en/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Auction system built in AWS Cloud infrastructure Auction web system built on Amazon Web Service cloud platform 1. Executive Summary The Auction system is designed by a FPTU student in Ho Chi Minh City and operates on the AWS Cloud platform. The platform utilizes AWS services to build an online auction marketplace with a user-friendly interface, easy to use and suitable for everyone.\n2. Problem Statement Current Problem Currently, auction systems have not reached many people due to difficulties in accessibility. This project was born to bring a transparent live auction platform that is friendly and accessible to everyone.\nSolution The platform uses AWS CloudFront and S3 Storage combined with ReactJS to provide the web interface, with EC2 servers handling all processing tasks on the Springboot platform, Amazon S3 for storing public and private data, and AWS RDS for database storage. Combined with Amazon Rekognition and Textract to extract information and verify user information to ensure fairness. With this platform, users can register new accounts, verify identities, and participate in exciting auctions on the platform.\nBenefits and Return on Investment (ROI) The project brings an online auction platform that is easily accessible to everyone. Estimated monthly cost is $59.37 USD (according to AWS Pricing Calculator). No additional development costs incurred.\n3. Solution Architecture The platform applies AWS architecture for data management. Public data is stored in public S3 buckets and displayed to users via CloudFront and S3 with ReactJS. All processing operations are performed on EC2 with the Springboot platform. Identity information is processed by Amazon Rekognition and Textract and then stored in private S3 buckets.\nAWS Services Used\nAWS VPC: Create a private virtual network environment. AWS Route 53: Route user traffic. AWS CloudFront: CDN helps accelerate page loading speed, reduce web access latency. AWS Load Balancing: Receive requests from the internet and route to EC2, stabilizing the application. Amazon EC2: Run springboot application to handle backend processing, communicate with database (RDS), cache queries (ElastiCache), call AI services (Rekognition, Textract) and process auctions. Amazon S3: Hosting Frontend: Store frontend source code (ReactJS, Tailwind) for CloudFront distribution. Data storage: 2 buckets (public/private) to store images uploaded by users for auctions and account verification. Amazon ElastiCache: Cache memory, helping store queries to reduce load for database and accelerate API response speed. Amazon RDS: Store main system data, placed in private subnet. Amazon Rekognition: AI image analysis service, performing Face Compare between selfie photos and ID photos to verify identity (eKYC). Amazon Textract: Text extraction service from documents. The system uses Textract to automatically read (OCR) and extract information from ID photos to automatically fill for users. Amazon SES: Backend uses this service to send account verification emails (OTP), auction winning notifications or other system notifications to users. Amazon CloudWatch: Service for monitoring and log management. Component Design\nData Ingestion: Data from users. Data Storage: Data stored in 2 S3 buckets (1 for public and 1 for private - accessed via presigned url) Data Processing: EC2 performs data processing. Web Interface: Amazon S3 stores ReactJS application. 4. Technical Implementation Implementation Stages The project includes the following stages:\nResearch and Architecture Design: Research and design AWS architecture, identify services to be used, design database. Cost Calculation and Feasibility Check: Use AWS Pricing Calculator to estimate and adjust. Development, Testing, Deployment: Program Springboot and ReactJS application, then test in local environment. Deployment on AWS Cloud Environment: Set up Gitlab CI, set up cloud environment and deploy. Technical Requirements\nJava 21 Springboot AWS SDK (S3, Rekognition, Textract, SES) MySQL RDS ReactJS/Vite/TypeScript/Tailwind Gitlab, Gitlab runner CI Postman, CloudWatch 5. Roadmap \u0026amp; Implementation Milestones Pre-internship (Month 0): 1 month for planning and evaluating the old station. Internship (Month 1–3): Month 1: Learn AWS and design architecture, design database, implement API construction. Month 2: Implement API construction, build interface. Month 3: Deploy on cloud environment, test, put into use. 6. Budget Estimate Costs can be viewed on AWS Pricing Calculator\nInfrastructure Costs\nAmazon Route53: $0.5/month (1 hosted zone) S3 Standard: $0.72/month (10 GB, 30000 request GET, 1000 request PUT, 5 GB Transfer out) Application Load Balancer: $18.80/month (data process 50GB) Amazon EC2 (t3.medium): $16.35/month (3yr, no upfront) Amazon ElastiCache (cache.t3.micro): $9.49/month (3yr, no upfront) Amazon RDS: $8.38/month Rekognition: $0.13/month (100 FaceCompare) Textract: $5/month (200 Pages document) Total: $59.37/month.\n7. Risk Assessment Risk Matrix\nNetwork Loss: High impact, low probability. Budget Overrun: Medium impact, low probability. Mitigation Strategy\nCost: AWS budget alerts, service optimization. Contingency Plan\nPeriodic backups in case of incidents. Use CloudFormation to restore cost-related configurations. 8. Expected Results Long-term Value: Can be reused for other projects in the future.\n"},{"uri":"https://huyle001.github.io/Hugo/en/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nAWS WELL-ARCHITECTED SECURITY PILLAR WORKSHOP Event Information Date \u0026amp; Time: Saturday, November 29, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent Purpose This morning workshop provided a comprehensive deep-dive into the AWS Well-Architected Security Pillar, covering all five security domains: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response.\nThe session was designed to equip participants with practical knowledge on implementing security best practices in AWS environments, with real-world examples from Vietnamese enterprises.\nAgenda Overview 8:30 – 8:50 AM | Opening \u0026amp; Security Foundation Security Pillar in Well-Architected Framework\nRole of Security Pillar in the Well-Architected Framework Core principles: Least Privilege – Zero Trust – Defense in Depth AWS Shared Responsibility Model Top cloud security threats in Vietnam Pillar 1 — Identity \u0026amp; Access Management 8:50 – 9:30 AM | Modern IAM Architecture\nIAM fundamentals: Users, Roles, Policies – avoiding long-term credentials IAM Identity Center: SSO, permission sets SCP \u0026amp; Permission Boundaries for multi-account environments MFA, credential rotation, Access Analyzer Mini Demo: Validate IAM Policy + simulate access Pillar 2 — Detection 9:30 – 9:55 AM | Detection \u0026amp; Continuous Monitoring\nCloudTrail (org-level), GuardDuty, Security Hub Logging at every layer: VPC Flow Logs, ALB/S3 logs Alerting \u0026amp; automation with EventBridge Detection-as-Code (infrastructure + rules) 9:55 – 10:10 AM | Coffee Break Pillar 3 — Infrastructure Protection 10:10 – 10:40 AM | Network \u0026amp; Workload Security\nVPC segmentation, private vs public placement Security Groups vs NACLs: application models WAF + Shield + Network Firewall Workload protection: EC2, ECS/EKS security basics Pillar 4 — Data Protection 10:40 – 11:10 AM | Encryption, Keys \u0026amp; Secrets\nKMS: key policies, grants, rotation Encryption at-rest \u0026amp; in-transit: S3, EBS, RDS, DynamoDB Secrets Manager \u0026amp; Parameter Store — rotation patterns Data classification \u0026amp; access guardrails Pillar 5 — Incident Response 11:10 – 11:40 AM | IR Playbook \u0026amp; Automation\nIR lifecycle according to AWS framework Playbooks: Compromised IAM key S3 public exposure EC2 malware detection Snapshot, isolation, evidence collection Auto-response with Lambda/Step Functions 11:40 – 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A Summary of 5 security pillars Common pitfalls \u0026amp; real-world practices in Vietnamese enterprises Security learning roadmap (Security Specialty, SA Pro certifications) Key Learnings Least Privilege is fundamental – always start with minimal permissions and expand as needed Zero Trust architecture assumes no implicit trust, even within the network Defense in Depth requires security controls at multiple layers Detection capabilities must be automated and continuous Incident response playbooks should be documented and tested regularly Application to My Work Implement IAM Access Analyzer in current projects Set up GuardDuty and Security Hub for centralized security monitoring Create incident response playbooks for common scenarios Review and tighten Security Group rules using least privilege Enable encryption for all data stores (S3, RDS, DynamoDB) Personal Experience This workshop was incredibly valuable for understanding AWS security holistically:\nThe practical demos made abstract concepts tangible Learning about common security pitfalls in Vietnamese enterprises was eye-opening The incident response playbooks provided actionable templates Understanding the relationship between all five pillars helps in designing comprehensive security architectures Takeaways Security is a continuous journey, not a destination Automation is key to maintaining security at scale The Well-Architected Security Pillar provides a comprehensive framework for cloud security Regular security reviews and improvements are essential AWS provides powerful native tools for each security domain Event Photos "},{"uri":"https://huyle001.github.io/Hugo/en/3-blogstranslated/3.3-blog3/","title":"Game Developer&#39;s Guide to Amazon DocumentDB Serverless","tags":[],"description":"","content":" Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGAME DEVELOPER\u0026rsquo;S GUIDE TO AMAZON DOCUMENTDB SERVERLESS Original Article: Game developer\u0026rsquo;s guide to Amazon DocumentDB Serverless\nOriginal Authors: Jackie Jiang, Douglas Bonser, and Matthew Nimmo\nPublication Date: 18 NOV 2025\nToday, we\u0026rsquo;ll explore how Amazon DocumentDB Serverless helps game developers streamline operations, handle unpredictable traffic, and optimize cost.\nWHAT IS AMAZON DOCUMENTDB SERVERLESS Amazon DocumentDB Serverless is an on-demand, auto scaling configuration for Amazon DocumentDB (with MongoDB compatibility). It automatically scales capacity up or down in fine-grained increments based on your application\u0026rsquo;s demand. It eliminates the need to manually provision, scale, and manage database instances while verifying you only pay for the resources you actually consume.\nWHY GAME STUDIOS SHOULD CARE For game studios managing player data, leaderboards, and game state, database management can be a significant operational headache. Amazon DocumentDB Serverless offers a compelling solution that\u0026rsquo;s particularly well-suited for the dynamic workloads common in gaming applications.\nLaunch day flexibility: There is no more guessing about what instance size is needed for launch day. Amazon DocumentDB Serverless automatically scales up to handle player surges, then scales down during quiet periods. Cost optimization: For seasonal games or those with distinct peak hours, you\u0026rsquo;re only paying for actual usage. When your European players are sleeping and your US players haven\u0026rsquo;t logged in yet, you\u0026rsquo;re not burning money on idle database capacity. Development environment efficiency: Testing new features or running QA environments? Amazon DocumentDB Serverless is perfect for development workloads where database usage is sporadic, but high performance is still crucial. REAL GAMING SCENARIOS Following are some gaming scenarios where Amazon DocumentDB Serverless could be of assistance:\nLive service games Example: During a season finale event in a battle royale game, player counts might spike from 50,000 to 500,000 in minutes. Amazon DocumentDB Serverless automatically scales to handle millions of concurrent inventory checks and match result writes. Daily reset management: When all players simultaneously claim daily rewards at reset time, the database seamlessly handles the surge without pre-provisioning. Special events: During a rare item drop event, the database can handle thousands of simultaneous inventory updates and marketplace transactions. Cross-platform MMOs Example: An MMO launches a new raid boss at different times across regions. As Asian servers peak, then European, then American, the database capacity follows the wave of player activity around the globe. Guild system data: Manages complex guild hierarchies, permissions, and shared inventories that require frequent updates and reads. Trading systems: Handles auction house transactions and player-to-player trading with consistent performance regardless of market activity. Mobile games with unpredictable growth Example: A casual mobile game suddenly goes viral on TikTok, growing from 10,000 to 1,000,000 daily active users in 48 hours. Amazon DocumentDB Serverless adapts automatically without developer intervention. Social features: Manages friend lists, gifting systems, and social interactions that can spike during promotional events. Season pass progress: Tracks and updates millions of players\u0026rsquo; battle pass progress simultaneously. UNDERSTANDING AMAZON DOCUMENTDB CAPACITY UNITS When a surge of players hits your game servers, Amazon DocumentDB Serverless automatically detects the increased load and scales compute power and memory. It measures usage in Amazon DocumentDB capacity units (DCUs). Each DCU is a combination of approximately 2 gibibytes (GiB) of memory, corresponding CPU, and networking.\nEach Amazon DocumentDB Serverless writer, or reader, has their capacity measured in DCUs. It is represented as a floating-point number that adjusts up or down with scaling. This capacity is updated every second. We recommend setting the minimum capacity to enable each writer, or reader, to keep the application\u0026rsquo;s working set in the buffer pool. This prevents the buffer\u0026rsquo;s content from being discarded during idle times.\nBefore adding Amazon DocumentDB Serverless instances to a cluster, the cluster must have the ServerlessV2ScalingConfiguration parameter set. This parameter defines the serverless instance capacity range with two values, MinCapacity and MaxCapacity, with a capacity range of 0.5256 DCUs. During a major guild raid or tournament, your database might need 64 DCUs to handle the load. During off-peak hours, it might scale down to 0.5 DCUs.\nAmazon DocumentDB Serverless continuously monitors CPU, memory, and network utilization (collectively called load) for each serverless writer or reader. This includes both application database operations, and background database and administrative tasks. When capacity limits are met or performance issues arise, Amazon DocumentDB Serverless automatically scales up to maintain optimal performance. Scaling increments start as small as 0.5 DCUs, with larger current capacities allowing bigger increments and faster scaling.\nWhen Amazon DocumentDB Serverless writers or readers are idle, instances can scale down to an idle state of 0.5 DCUs if MinCapacity is set to 0.5. In this state the compute resources are insufficient for most production workloads, but the instances remain ready to quickly scale up. Upon exiting an idle state, instances scale directly to at least 1.02.5 DCUs (or the MaxCapacity, if lower). To enable scaling down to 0.5 DCUs, instance capacity is limited whenever MinCapacity is 1.0 DCUs or less.\nThe charges for Amazon DocumentDB Serverless capacity are measured in terms of DCU hours. By billing for each DCU usage, instead of maintaining high-capacity instances for peak times, serverless databases align their costs with actual player activity. This frees your development team to focus on building game features rather than forecasting and managing infrastructure.\nREQUIREMENTS AND CONSIDERATIONS Amazon DocumentDB Serverless requires that clusters are running engine version 5.0.0, and it is not supported in older engine versions. While some Amazon DocumentDB features are compatible with Amazon DocumentDB Serverless, they may cause performance issues or out-of-memory errors if your capacity range is insufficient for your workload\u0026rsquo;s memory requirements.\nFeatures requiring higher MinCapacity or MaxCapacity settings include Performance Insights and serverless instance creation on clusters with large data volumes (including during cluster restoration). If you encounter Out-of-memory errors due to capacity misconfiguration read Avoiding out-of-memory errors for more information. Amazon DocumentDB Serverless instances do not support global clusters.\nMIGRATING TO SERVERLESS Each Amazon DocumentDB cluster can use serverless capacity, provisioned capacity, or both in a mixed configuration. For example, combine a large, provisioned writer with serverless readers for higher write throughput, or use a serverless writer with provisioned readers when writes are variable, but reads are steady. It\u0026rsquo;s also possible to configure a cluster to use only serverless, either by creating a new serverless cluster or converting existing provisioned capacity.\nTo migrate to Amazon DocumentDB Serverless, first upgrade the cluster to the supported engine version 5.0.0. Configure scaling by adding serverless instances and optionally perform a failover operation to make a serverless instance the cluster writer. You then optionally convert provisioned instances to serverless or remove them.\nMigration from MongoDB requires Amazon Web Services (AWS) Database Migration Service (AWS DMS) for online, minimal-downtime migrations, or native MongoDB tools for offline dump and restore. Review Migrating to Amazon DocumentDB Serverless for detailed step-by-step guidance and best practices.\nAfter migration, follow Amazon DocumentDB best practices by optimizing indexes, monitoring key performance metrics, enforcing security and cost controls. Be certain to tune your workloads to confirm reliability, efficiency, and scalability.\nMONITORING SERVERLESS USAGE Amazon DocumentDB Serverless provides granular monitoring through Amazon CloudWatch metrics which update every second to give real-time insight into instance scaling and resource usage. Typically, most scaling up for Amazon DocumentDB Serverless instances is caused by memory usage and CPU activity. Alternatively, you can use Performance Insights to monitor the performance of Amazon DocumentDB Serverless instances.\nAmazon DocumentDB Serverless offers four critical monitoring metrics to help you track performance and optimize resource allocation:\nServerlessDatabaseCapacity: Measures the current capacity of each instance in DCUs, reflecting CPU, memory, and networking resources. At the cluster level, it averages the capacity across all serverless instances.\nDCUUtilization: Measures the percentage of current database capacity used relative to the maximum configured capacity. If it nears 100%, the instance has scaled to its limit, and users should consider increasing the max DCU or adding more reader instances to balance workloads.\nCPUUtilization: Represents the percentage of CPU currently used relative to the CPU capacity available under the cluster\u0026rsquo;s maximum DCU setting. It monitors CPU usage continuously and triggers automatic scaling when usage is consistently high.\nFreeableMemory: Represents the amount of unused memory available when the instance is scaled to its maximum capacity, increasing by about 2 GiB for each DCU below the max capacity.\nSUMMARY Amazon DocumentDB Serverless delivers streamlined elasticity that game developers need by eliminating pre-sizing guesswork, handling event surges, and aligning costs with existing player activity. By automatically scaling database capacity based on actual demand, Amazon DocumentDB Serverless provides game developers with a way to focus on creating engaging player experiences, while AWS handles the database infrastructure complexity.\nContact an AWS for Games specialist to know how we can help accelerate your business.\nFURTHER READING Amazon DocumentDB Serverless is now available Using Amazon DocumentDB Serverless JSON database solutions in AWS: Amazon DocumentDB (with MongoDB compatibility) Game Developer\u0026rsquo;s Guide to Amazon DocumentDB Part 1: An Overview Choosing the scaling capacity range for a DocumentDB serverless cluster ABOUT THE AUTHORS Jackie Jiang is a Principal Solutions Architect at AWS. Jackie works with enterprise gaming customers to support their innovation in the cloud. She leverages 20+ years of cross-industry experience to help her customers bring their visions to life in a reliable, scalable, and cost-effective manner.\nDouglas Bonser is a Senior DocumentDB Specialist Solutions Architect. He brings 30+ years working extensively with NoSQL and relational database technologies. Douglas enjoys helping customers solve problems and modernize their applications using NoSQL database technology.\nMatthew Nimmo is a Solutions Architect for Game Tech at AWS. As both a technologist and avid gamer, he helps game studios leverage cloud technology to solve complex challenges and create better player experiences.\n"},{"uri":"https://huyle001.github.io/Hugo/en/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"TRANSLATED BLOGS ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section lists and introduces the AWS blogs that have been translated from English to Vietnamese as part of the FCJ internship program.\nBlog 1 - Authentication for Mobile Games This blog explores the critical challenges of implementing authentication and authorization systems for mobile games. You will learn about common obstacles such as supporting anonymous users, creating frictionless sign-up processes, handling offline authentication, and protecting against client tampering. The article provides comprehensive guidance on what to look for in an authentication solution, including security requirements (TLS v1.2+, encrypted storage), user experience considerations, scalability needs, and best practices like utilizing app store protection mechanisms (Play Integrity API, DeviceCheck) and following OAuth 2.0 security standards.\nBlog 2 - Making SaaS Products Accessible in AWS Marketplace This comprehensive guide explains the three main access options for SaaS sellers in AWS Marketplace. You will discover how to use the seller\u0026rsquo;s website with AWS Marketplace serverless integration, Quick Launch implementation using pre-configured CloudFormation templates, and AWS PrivateLink configuration for enhanced security. Each option includes detailed implementation steps, workshop resources, and architectural diagrams showing how ISV provider accounts connect to customer consumer accounts through various access methods.\nBlog 3 - Game Developer\u0026rsquo;s Guide to Amazon DocumentDB Serverless This blog introduces how Amazon DocumentDB Serverless helps game developers handle unpredictable player traffic while optimizing costs. You will learn about DocumentDB Capacity Units (DCUs), automatic scaling mechanisms that adjust from 0.5 to 256 DCUs based on actual demand, and real gaming scenarios including live service games with season finale events, cross-platform MMOs with global raid boss launches, and mobile games that go viral on TikTok. It covers migration strategies from MongoDB, CloudWatch monitoring metrics, and best practices for ensuring reliability and scalability.\nBlog 4 - Monitoring Server Health with Amazon GameLift Servers This technical guide demonstrates how to use Amazon GameLift Servers telemetry metrics with Amazon Managed Grafana dashboards to diagnose game server issues. You will learn how to troubleshoot game server crashes by analyzing memory usage patterns, identify which specific game sessions are consuming excessive resources, and investigate high CPU usage that causes stuttering gameplay. The article walks through seven pre-built Grafana dashboards and explains how to implement custom metrics for game-specific health indicators like combat balance, progression blockers, and economy health.\nBlog 5 - Strengthen Foundation Model Queries Through Amazon Bedrock-Amazon Alexa Integration This blog presents a solution developed for the Federal Institute of São Paulo (IFSP) to generate SQL queries from natural language questions using Amazon Bedrock and Alexa. You will learn how Claude 3 Haiku model addresses the hallucination problem when dealing with structured data by generating SQL code instead of directly interpreting tables. The article details the complete architecture including Alexa Skills Kit, AWS Lambda, Amazon Athena, AWS Glue Data Catalog, and Amazon EventBridge. It covers prompt engineering techniques, security implementation with IAM roles and policies, and data encryption using AWS KMS.\nBlog 6 - Wicked Saints Studios Integrates TikTok Within World Reborn Using AWS This case study shows how Wicked Saints Studios built a TikTok authentication integration for their mobile game World Reborn using the Guidance for Custom Game Backend Hosting on AWS. You will learn how they overcame challenges including deep link encoding, redirect allowlist mismatches, and extended TikTok API approval times. The article presents the complete OAuth flow implementation with C# code examples for server endpoints, Unity client integration using deep links and REST APIs, and security measures including token encryption in AWS Secrets Manager and metadata storage in Amazon DocumentDB.\n"},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.4-distribution/5.4.3-cloudfront/","title":"Amazon CloudFront","tags":[],"description":"","content":"Amazon CloudFront CloudFront helps distribute static content from S3 with low latency.\nSteps Access CloudFront Console -\u0026gt; Create distribution.\nIn Origin domain, select the Frontend S3 Bucket. In Origin access, select Legacy access identities or OAC (Origin Access Control) to restrict direct user access to S3. Select Create new OAC.\nViewer protocol policy: Redirect HTTP to HTTPS.\nAllowed HTTP methods: GET, HEAD, OPTIONS.\nWAF: Do not enable (to save costs). Alternate domain name (CNAME): Enter frontend domain (e.g., www.example.com).\nCustom SSL certificate: Select ACM certificate. Default root object: index.html. Click Create distribution.\nAfter creation, remember to update the Bucket Policy of S3 to allow OAC access (Console will suggest copying the policy).\n"},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.2-database-storage/5.2.3-s3/","title":"Amazon S3","tags":[],"description":"","content":"Initialize Amazon S3 Buckets We need 3 buckets for different purposes.\n1. Frontend Bucket Used for static web hosting (ReactJS).\nCreate a bucket with a unique name. Uncheck Block all public access (as the web needs to be public). Enable Static website hosting. 2. Public Storage Bucket Stores auction product images (public read).\nCreate a bucket. Uncheck Block all public access. Configure Bucket Policy to allow s3:GetObject. 3. Private Storage Bucket Stores identity documents for account verification (private).\nCreate a bucket. Keep Block all public access checked. (Optional) Configure Server-side encryption. "},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.3-compute/","title":"Compute","tags":[],"description":"","content":"Initialize Amazon EC2 Amazon EC2 (Elastic Compute Cloud) will be where the backend application (Spring Boot) runs.\nContent EC2 Instance: Virtual server running the application. Security Group: Firewall controlling access. Elastic IP: Static IP address (optional, necessary if not using a Load Balancer or need a fixed outbound IP). "},{"uri":"https://huyle001.github.io/Hugo/en/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"EVENTS PARTICIPATED ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nEvent 1 Event Name: AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: Saturday, November 15, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nDescription: Hands-on workshop covering Amazon SageMaker for traditional ML workflows and Amazon Bedrock for generative AI applications, including RAG architecture, prompt engineering, and Bedrock Agents.\nEvent 2 Event Name: DevOps on AWS Workshop\nDate \u0026amp; Time: Monday, November 17, 2025, 8:30 AM – 5:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nDescription: Comprehensive full-day workshop covering AWS DevOps services including CI/CD pipelines with CodePipeline/CodeBuild/CodeDeploy, Infrastructure as Code with CloudFormation and CDK, container services (ECS, EKS, App Runner), and monitoring \u0026amp; observability with CloudWatch and X-Ray.\nEvent 3 Event Name: AWS Well-Architected Security Pillar Workshop\nDate \u0026amp; Time: Saturday, November 29, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nDescription: Morning workshop providing a comprehensive deep-dive into the AWS Well-Architected Security Pillar, covering all five security domains: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response.\n"},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.4-distribution/5.4.4-route53/","title":"Amazon Route 53","tags":[],"description":"","content":"Amazon Route 53 Configure DNS to point domain to CloudFront (Frontend) and ALB (Backend).\nSteps Access Route 53 -\u0026gt; Hosted zones. Select your domain. Route53 will provide 4 NS records, configure them at your domain registrar to manage the domain in Route53. This process takes a few minutes. 1. Point to Backend (ALB) Create record. Record name: api (e.g., api.example.com). Record type: A. Enable Alias. Route traffic to: Alias to Application and Classic Load Balancer. Select Region and your ALB. 2. Point to Frontend (CloudFront) Create record. Record name: www or leave blank (root domain). Record type: A. Enable Alias. Route traffic to: Alias to CloudFront distribution. Select CloudFront distribution. "},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.4-distribution/","title":"Distribution","tags":[],"description":"","content":"Content Distribution This section helps deliver the application to end users securely and with high performance.\nContent Certificate (ACM): Issue free SSL/TLS certificates. Application Load Balancer (ALB): Load balance for Backend (Spring Boot). CloudFront: CDN to distribute Frontend (ReactJS) and Static files (S3). Route 53: Manage Domain DNS. "},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.5-deploy/","title":"CI/CD","tags":[],"description":"","content":"Continuous Integration and Deployment (CI/CD) Use GitLab CI to automate the process of building and deploying the application to AWS.\nContent SSH into EC2 instance to install necessary applications. GitLab Runner: Configure runner on EC2 (or use Shared Runner). Pipeline: Define .gitlab-ci.yml file. "},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/","title":"System Deployment","tags":[],"description":"","content":"Deploy Auction System on AWS Welcome to the workshop on deploying an online auction system on the AWS platform. In this workshop, we will build each component of the system together based on the proposed architecture.\nObjectives Complete the installation and configuration of necessary AWS services to operate the Auction System.\nArchitecture We will follow this architecture:\nSteps Preparation: Setup VPC, IAM Role. Database \u0026amp; Storage: Configure RDS, ElastiCache, S3. Compute: Install and configure EC2. Distribution: Configure Load Balancer, CloudFront, Route 53. CI/CD: Setup automated deployment process with GitLab CI. Clean up: Delete resources after completion. "},{"uri":"https://huyle001.github.io/Hugo/en/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at Amazon Web Services (AWS) from September 8, 2025 to December 16, 2025, I gained valuable experience working in a professional cloud computing environment. I participated in the First Cloud Journey workshop program and contributed to developing the project, which helped me enhance my abilities in Cloud Computing, AWS Services, Serverless Architecture, and collaborative work.\nThroughout the internship, I maintained a strong work ethic by consistently delivering quality results, following organizational guidelines, and collaborating with team members to achieve common goals.\nBased on my experience during this period, I would like to provide an honest self-assessment using the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Continue developing technical expertise in cloud technologies and deepen understanding of AWS services Strengthen proactive approach in identifying and proposing solutions to technical challenges Enhance communication skills, especially in presenting technical concepts and documentation Improve time management and adherence to established workflows and schedules "},{"uri":"https://huyle001.github.io/Hugo/en/5-workshop/5.6-cleanup/","title":"Clean Up","tags":[],"description":"","content":"Clean Up Resources To avoid unwanted costs after completing the workshop, please delete resources in the following order:\nDeletion Order EC2: Terminate instances. RDS \u0026amp; ElastiCache: Delete database and cache cluster. Delete Subnet Groups and Snapshots. Load Balancer \u0026amp; Target Group: Delete ALB first, then Target Group. CloudFront: Disable distribution, wait for deployment to finish, then Delete. S3: Empty bucket (delete all objects) then Delete bucket. NAT Gateway \u0026amp; Elastic IP: Delete NAT Gateway -\u0026gt; Release Elastic IP. VPC: Delete VPC (this will automatically delete Subnets, Internet Gateway, Route Tables, and related Security Groups). Note: Check the Billing Dashboard the next day to ensure there are no incurring costs.\n"},{"uri":"https://huyle001.github.io/Hugo/en/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nOverall Evaluation 1. Working Environment\nThe working environment at First Cloud Journey is quite welcoming and supportive. FCJ team members are helpful when I face challenges, and they respond even beyond regular hours. The learning space is well-organized and comfortable, which allows me to concentrate better on studying AWS concepts.\nI think adding more informal meetups or sharing sessions between participants would help strengthen connections and create better opportunities for exchanging experiences.\n2. Support from Mentor / Team Admin\nThe mentor offers detailed explanations and consistently encourages questions whenever something isn\u0026rsquo;t clear. What I value most is the approach of letting me explore and troubleshoot AWS services independently before providing direct answers. This helps develop my problem-solving abilities.\nThe admin team handles documentation, schedules, and program logistics smoothly, which makes the whole experience more convenient and less stressful.\n3. Relevance of Work to Academic Major\nThe program content connects well with my university studies, particularly in areas like systems architecture, networking fundamentals, and cloud infrastructure. Through FCJ, I gained hands-on exposure to AWS services such as EC2, S3, IAM, and VPC in practical scenarios.\nThis experience reinforced my theoretical foundation while introducing me to real-world implementations that aren\u0026rsquo;t typically covered in classroom settings.\n4. Learning \u0026amp; Skill Development Opportunities\nThe program offers valuable learning experiences in several areas:\nFamiliarity with project collaboration and management approaches Development of teamwork capabilities through group activities Enhancement of analytical thinking when working with AWS architecture Improvement in technical communication and documentation skills The mentor shared useful insights about cloud career paths, certification preparation strategies, and industry best practices that extend beyond the core curriculum.\n5. Company Culture \u0026amp; Team Spirit\nThe program maintains a constructive atmosphere where participants support each other\u0026rsquo;s learning journey. Despite being a student intern, I felt welcomed and included in the community.\nDuring group projects or when deadlines approached, everyone collaborated effectively, sharing knowledge and helping one another, which fostered a genuine sense of belonging.\n6. Internship Policies / Benefits\nThe program provides comprehensive learning materials, consistent mentorship, a flexible study arrangement, and financial support that varies by program phase.\nAccess to internal AWS training sessions is particularly beneficial, offering practical knowledge that complements theoretical learning.\nAdditional Questions What did you find most satisfying during your internship?\nThe hands-on practice with AWS services and the supportive learning environment where I could experiment without fear of making mistakes. The mentor\u0026rsquo;s practical guidance was invaluable.\nWhat do you think the program should improve for future participants?\nMore structured peer-learning sessions and perhaps additional practice labs for complex AWS scenarios would be helpful. Also, clearer roadmaps for different skill levels could benefit diverse participant backgrounds.\nIf recommending to a friend, would you suggest they join this program? Why or why not?\nYes, I would recommend it. The program offers solid practical AWS experience, supportive mentorship, and a good foundation for those interested in cloud computing careers. It\u0026rsquo;s especially suitable for students wanting to bridge academic knowledge with industry practices.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nOrganize more peer discussion sessions for knowledge exchange Provide additional reference materials for different learning styles Consider creating a community forum for ongoing questions and discussions Would you like to continue this program in the future?\nYes, I\u0026rsquo;m interested in continuing with more advanced AWS topics and possibly pursuing certification preparation programs if available.\nAny other comments (free sharing):\nOverall, this has been a meaningful learning experience. The combination of theoretical knowledge and practical application helped me understand cloud computing better. I appreciate the team\u0026rsquo;s effort in creating a supportive learning environment for students.\n"},{"uri":"https://huyle001.github.io/Hugo/en/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://huyle001.github.io/Hugo/en/tags/","title":"Tags","tags":[],"description":"","content":""}]